---
title: Estudio sobre la empleabilidad de los estudiantes
author:
    - Sergio Quijano Rey
    - sergioquijano@correo.ugr.es
date:
output:
    html_document:
        toc: true
        toc_depth: 3
        toc_float: true
        collapsed: true
        number_sections: true
---

# Introducción

Vamos a trabajar con una base de datos sobre la empleabilidad de ciertos estudiantes tras hacer una entrevista de trabajo, a modo de entrenamiento para su futura búsqueda de trabajo.

Tenemos dos objetivos en este trabajo:

1. Construir un modelo que prediga, en base a las métricas recogidas durante la entrevista, si una persona es empleable o no
2. Estudiar las métricas recogidas, buscando ver cuáles son las más relevantes <!-- TODO -- quizas esto es mentira por lo que voy a hacer mas tarde -->

Notar que todo el código, así como el historial del desarrollo de este, está disponible en [este repositorio de Github](https://github.com/SergioQuijanoRey/EstadisticaMultivariantePracticaFinal)

# Paquetes utilizados

A continuación, importamos todos los paquetes que vamos a utilizar:

```{r}
# Para leer archivos .sav
library(foreign)

# Para leer archivos excel
library(readxl)

# Para hacer ciertos gráficos
library(tidyr)
library(ggplot2)
```

Estos paquetes se pueden instalar con la orden `install.packages("<nombre_paquete>")`. Sin embargo, hemos definido todos los paquetes necesarios, de forma declarativa, en el archivo `shell.nix` que se puede encontrar en [este archivo](https://github.com/SergioQuijanoRey/EstadisticaMultivariantePracticaFinal/blob/main/shell.nix). Para usar este entorno de paquetes se puede consultar la [documentación oficial de Nix](https://nixos.org/)

# Descripción de la base de datos

La base de datos se puede encontrar en el repositorio [Kaggle](https://www.kaggle.com/datasets/anashamoutni/students-employability-dataset). Este *dataset* contiene datos recogidos por agencias universitarias en las Filipinas. Contiene datos de entrevistas de trabajo de pruebas (para entrenar a los estudiantes en su futura búsqueda de trabajo). El *dataset* no ha sido limpiado ni procesado ^[Según lo que se dice el Kaggle, sin embargo más adelante comentamos que esto no es del todo cierto], así que este trabajo lo tendremos que realizar nosotros.

Este *dataset* se compone de 10 columnas y 2983 filas. Las columnas se corresponden con:

1. Identificador numérico del estudiante. Lo llaman nombre del estudiante, pero en verdad se corresponde con una etiqueta numérica del tipo "Student <id>"
2. Apariencia general
3. Formas de hablar
4. Condición física
5. Agilidad mental
6. Confianza en si mismo
7. Habilidad para presentar ideas
8. Habilidades de comunicación
9. Rendimiento académico
10. Empleabilidad

Todas estas variables, salvo el identificador y la empleabilidad, son valores discretos del 1 al 5. La estructura del identificador ya la hemos comentado. La empleabilidad es una variable de tipo texto con los valores `"Employable"` y `"LessEmployable"`. Esta es la variable que queremos predecir a partir de las métricas recogidas durante la entrevista.

Si nos fijamos en el número de filas y los identificadores de los estudiantes, podemos ver que tenemos hasta el identificador 3000, mientras que tenemos 22 filas menos. Esto nos hace pensar que sí que se han procesado los datos, eliminando 22 filas o bien con datos faltantes o bien con *outliers*.

Como veremos más adelante, el *dataset* no presenta datos faltantes, así que lo más seguro es que las filas eliminadas sean por los datos faltantes.

Con todo esto, ya estamos en condiciones de importar los datos:

```{r}
df <- read_excel("../data/Student-Employability-Datasets.xlsx")
head(df)
```

Y ahora, un resumen del *dataset*:

```{r}
summary(df)
```

A partir del resumen, podemos ver que:

- Aunque las métricas van, en un diseño inicial, del valor 1 al 5, muchas de ellas tienen un rango más acotado (por debajo), puesto que no hubo participantes que obtuviesen los valores mínimos.
- Todas las variables tienen, como valor mínimo, el 2. Solo la variable `Student Performance Rating` tuvo un valor mínimo de 3


# Análisis Exploratorio Univariante

## Procesamiento inicial de los datos

Lo primero que tenemos que hacer es eliminar la primera fila del *dataset*. Esta se corresponde con un identificador de los estudiantes, que no nos va a ser útil para nada:

```{r}
df <- df[, -1]
head(df)
```

Por comodidad, cambiamos el nombre de las columnas de datos:

```{r}
colnames(df) <- c("APARIENCIA", "FORMAS HABLAR", "CONDICION FISICA", "AGILIDAD MENTAL", "CONFIANZA", "HAB. PRESENTAR IDEAS", "HABILIDADES COMUNICACION", "RENDIMIENTO ACADEMICO", "CLASS")
head(df)
```

## Separación en training / test

Para realizar una posterior validación cruzada, tenemos que separar los datos en dos conjuntos, uno de entrenamiento y otro de test. Además, todo el procesamiento de datos que realicemos, deberá ser de la siguiente forma, para evitar caer en *data snooping*: ^[Para un lector que no esté familiarizado con este concepto, se puede consultar [este recuros](https://web.ma.utexas.edu/users/mks/statmistakes/datasnooping.html)]

- Todo el procesamiento de datos debe aprenderse sobre el conjunto de entrenamiento
- Este procesamiento aprendido en el conjunto de entrenamiento debe aplicarse ciegamente sobre el conjunto de test
- Por ejemplo, si se decide estandarizar una variable, se debe aplicar usando la media y desviación del conjunto de entrenamiento. Ese valor, sobre el conjunto de entrenamiento, se aplica después en la normalización del conjunto de test, y no los estadísticos en test

Algunas operaciones, por su naturaleza, no tienen por qué ser tan estrictas. Por ejemplo, una recodificación de una variable de tipo texto en tipo numérico de forma sencilla (como hacemos en [Recodificaciones o agrupaciones de datos]).

Dicho esto, separamos en los dos conjuntos de datos. Hacemos un muestreo aleatorio porque en muchos conjuntos de datos, el orden de los datos involucra cierto patrón:

```{r}
# Estableciendo la semilla, hacemos que los resultados sean reproducibles
set.seed(123456789)

# Cuantos datos queremos en training
train_percentage <- 0.80
train_size <- floor(train_percentage * nrow(df))

# Muestreamos los indices de training
train_indixes <- sample(seq_len(nrow(df)), size = train_size)

# Con los indices, hacemos la separacion
df_train <- df[train_indixes, ]
df_test <- df[-train_indixes, ]

# Vemos los tamaños de los datasets obtenidos

message("Conjunto de entrenamiento")
message(ncol(df_train), " , ", nrow(df_train))

message("Conjunto de test")
message(ncol(df_test), " , ", nrow(df_test))
```

## Recodificaciones o agrupaciones de datos

Por lo que hemos visto previamente, la variable que queremos predecir toma dos valores de tipo texto. Muchos de los métodos de clasificación trabajan (solo, o de forma mejor) con variables de tipo numérico. Así que realizamos el siguiente cambio:

- `Employable -> 1`
- `LessEmployable -> 0`

Además, haremos que la variable numérica sea un factor, porque ciertas funciones de `R` necesitan esto para trabajar de forma correcta.

```{r}
# Aplica un cambio de variable a la variable objetivo
# `Employable -> 1`
# `LessEmployable -> 0`
change_target_format <- function(target){
    if (length(target) > 1){
        stop("Se ha pasado un vector en vez de un unico valor, que es lo que espera la funcion")
    }


    if(target == "Employable"){
        return(1)
    } else if (target == "LessEmployable"){
        return(0)
    }

    # Esta situacion nunca deberia pasar
    # Tenemos una variable que no toma ninguno de los dos valores supuestos
    stop("La variable pasada no es ni 'Employable' ni 'LessEmployable'")
}

# Aplicamos dicha funcion a la salida
df_train$CLASS <- sapply(df_train$CLASS, change_target_format)

# Aplicamos un factor a la variable de salida
df_train <- within(df_train, {
    CLASS <- factor(CLASS, labels = c("Empleable", "No empleable"))
})

# Aplicamos el mismo procesamiento al dataframe de test
df_test$CLASS <- sapply(df_test$CLASS, change_target_format)

# Aplicamos un factor a la variable de salida
df_test <- within(df_test, {
    CLASS <- factor(CLASS, labels = c("Empleable", "No empleable"))
})
```

Veamos algunos datos de entrenamiento, ahora que hemos hecho la modificación:

```{r}
# Mostramos algunas entradas de los datos tras haber hecho la recodificacion de
# la variable de salida
head(df_train)
```

Y con el resumen del dataframe podemos ver que el cambio ha sido efectivo, a nivel del tipo de datos que estamos manejando:

```{r}
summary(df_train)
```

El resto de variables son numéricas discretas, que toman un rango de valores muy pequeño, y por lo tanto no parece necesario que las agrupemos.

## Valores perdidos

Como ya hemos comentado en [Descripción de la base de datos], sospechamos que el autor de la base de datos ya ha realizado la limpieza de los datos faltantes. A continuación, nos aseguramos de que no haya datos faltantes, ni en entrenamiento ni en test:

```{r}
sapply(
    df_train,
    function(col) {
        sum(is.na(col))
    }
)

sapply(
    df_test,
    function(col) {
        sum(is.na(col))
    }
)
```

Efectivamente, no tenemos valores perdidos, ni en entrenamiento ni en test. Ahora, como suponemos, por lo comentado previamente, que el *dataset* original tenía 3000 entradas, y nos quedamos con 22 menos, tenemos un porcentaje de valores perdidos:

```{r}
missing_values_perc <- 22 / 3000 * 100
missing_values_perc
```

Esto es algo menos del 0.74%, por lo tanto, no es necesario analizar el patrón aleatorio de los valores perdidos. Además, el procesamiento de estos valores perdidos ya ha sido impuesto por el autor de la base de datos, que ha decidido eliminarlos. Esto no nos supone un gran problema puesto que tenemos un volumen de datos considerable

## Análisis Descriptivo Numérico Clásico

El primer paso es observar la información que nos da la función `summary`. Aunque ya la hemos visto muchas veces, volvemos a consultarla:

```{r}
summary(df_train)
```

Veamos también los gráficos de cajas de cada una de las variables. Con esto, será más sencillo comentar los resultados que nos da la función `summary`, que nos da información sobre cuartiles, mínimos, máximos, mediana y media:

```{r}
boxplot(
    df_train,

    # Para que los nombres de las variables salgan en vertical
    las = 2,

    # Doy nombres acortados para que se vea mejor
    names = c("APARIENCIA", "FORM.HABLAR", "COND.FIS", "AGIL.MENTAL", "CONFIANZA", "PRESENTAR IDEAS", "COMUNICACION", "ACADEMICO", "CLASS")
)
```

A partir de esto, podemos ver que:

1. Como ya hemos comentado en [Descripción de la base de datos], ninguna de las variables toma el valor 1, aunque el diseño del estudio se hubiese hecho teniendo esto en mente, y el rendimiento académico tiene como valor mínimo el valor 3
2. Casi todas las variables tienen valores altos (en la escala $1-5$). Son destacables las variables apariencia física y rendimiento académico
    - Es de esperar que la mayoría de personas acudan a la entrevista de trabajo con una buena aparencia
    - Al ser un estudio realizado sobre personas universitarias, es de esperar que su rendimiento sea alto. Además, son universitarios que en un futuro cercano van a estar buscando trabajo, y por tanto estarán a punto de terminar sus estudios universitarios, lo que puede hacer que el rendimiento sea aún más sesgado a lo alto
3. Las variables más variadas son la condición física, la agilidad mental y la confianza en uno mismo
4. La habilidad comunicativa es aquella con mediana más baja (sin contar la variable de salida, que tenemos que considerar por separado)
5. Al estar trabajando con variables discretas, no podemos obtener una información tan fina con esta información como sí podríamos obtener con variables continuas. Además, las variables están en los mismos rangos $1-5$, así que estamos obligados a usar otros recursos para obtener información, previo al ajuste de un modelo, a partir de estos datos
6. Aunque lo veremos más adelante en [Balanceo de las clases], podemos ver que la clase "No empleable" es la predominante

Veamos ahora los histogramas de las variables:

```{r}
# Esta grafica la copiamos, sin apenas modificaciones, de:
# https://statisticsglobe.com/histogram-density-for-each-column-data-frame-r
data_long <- df_train[, -9] %>%
    pivot_longer(colnames(df_train[, -9])) %>%
    as.data.frame()

ggp1 <- ggplot(data_long, aes(x = value)) +
    geom_histogram(bins = 10) +
    facet_wrap(~ name, scales = "free")

ggp1
```

Con esta gráfica, podemos ver que:

1. Muchas de las variables (por ejemplo, agilidad mental, condición física, habilidad para presentar ideas, ...) parecen seguir una distribución normal. Esto lo comprobaremos más adelante con un contraste de hipótesis
2. Tanto el rendimiento académico, como las habilidades de comunicación, no son simétricas
    - En el caso de las habilidades de comunicación, con más concentración hacia la izquierda. Esto ya lo intuíamos al ver los gráficos de bigotes. La habilidad de comunicación es la que peor se les da a los candidatos, en relación a las otras variables
    - En el caso del rendimiento académico, casi todos los candidatos tienen una valoración de 5. La variable de la que podríamos esperar mayor relevancia a la hora de valorar la empleabilidad de un candidato, es de las que menos variabilidad presenta
3. Casi todas las variables se concentran en el rango $[3, 5]$, como ya sospechábamos previamente al ver un resumen rápido de los datos

## Outliers

En las gráficas de cajas anteriores, en [Análisis Descriptivo Numérico Clásico], hemos visto que la única variable que presenta *outliers* es la apariencia. Pero veamos esto de forma más clara usando el siguiente código (que se basa en el entregado en las *Entregas Voluntarias* de *PRADO*):

```{r}

# Devuelve el numero de outliers que tiene una columna de datos
how_many_outliers <- function(data) {
    H <- 1.5 * IQR(data)
    outliers <- which(
        data < quantile(data, 0.25, na.rm = TRUE) - H |
        data > quantile(data, 0.75, na.rm = TRUE) + H
    )
    return(length(outliers))
}

# Comprueba si una columna de datos tiene outliers
has_outliers <- function(data) {
    return(how_many_outliers(data) > 0)
}

# No miramos la variable de salida, porque no podemos calcular cuartiles sobre factores
# Ademas, no tiene sentido buscar valores atípicos en la salida que queremos predecir
sapply(df_train[, -9], has_outliers)
```

Vemos que, en efecto, solo la variable de la apariencia presenta *outliers*. Ahora, veamos cuántos *outliers* tenemos en dicha variable. Si son relativamente pocos, los eliminaremos. En otro caso, tendremos que sustituirlos por el valor de la media:

```{r}
no_outliers <- how_many_outliers(df_train$APARIENCIA)
perc_outliers <- no_outliers / length(df_train$APARIENCIA) * 100
message("Nº outliers: ", no_outliers, " --> ", perc_outliers, "%")
```

El porcentaje de filas con *outliers* está por debajo del 0.6%, y teniendo en cuenta el volumen de datos, procedemos a borrar aquellas filas con un *outlier* unidimensional en la variable de apariencia (de nuevo, usando un código que hemos adaptado del entregado en la *Entrega Voluntaria*). Aquí es crítico aplicar lo que hemos comentado previamente sobre *data snooping*.

```{r}

# Metricas, que calculamos sobre el conjunto de train, para aplicar el borrado
# de outliers
quantiles <- quantile(df_train$APARIENCIA, probs = c(0.25, 0.75), na.rm = FALSE)
H <- IQR(df_train$APARIENCIA) * 1.5
lower <- quantiles[1] - H
upper <- quantiles[2] + H

# Con estos estadisticos, borramos los outliers en train
rows_prev <- nrow(df_train)
df_train <- subset(df_train, df_train$APARIENCIA > lower & df_train$APARIENCIA < upper)
rows_deleted <- rows_prev - nrow(df_train)
message("TRAIN, hemos borrado ", rows_deleted, " filas, lo que supone un ", rows_deleted / rows_prev * 100, "%")

# Usando los mismos estadisticos, para evitar hacer data snooping, borramos
# outliers en test
rows_prev <- nrow(df_test)
df_test <- subset(df_test, df_test$APARIENCIA > lower & df_test$APARIENCIA < upper)
rows_deleted <- rows_prev - nrow(df_test)
message("TEST, hemos borrado ", rows_deleted, " filas, lo que supone un ", rows_deleted / rows_prev * 100, "%")
```

Ahora, podemos comprobar que no quedan *outliers* en el conjunto de entrenamiento:

```{r}
sapply(df_train[, -9], has_outliers)
```

Y en efecto, no tenemos ningún outlier. Veamos (aunque se pueda saber con los datos mostrados previamente), el tamaño de nuestros conjuntos de datos tras los borrados:

```{r}
nrow(df_train)
nrow(df_test)
```

Como era de esperar (pues hemos borrado porcentajes muy bajos de datos), seguimos teniendo conjuntos de datos grandes con los que trabajar.

## Balanceo de las clases

Esto se ha podido ver en operaciones realizadas anteriormente sobre la base de datos, pero lo hacemos ahora explícitamente. Solo lo hacemos para el conjunto de entrenamiento. Como hemos hecho un muestreo aleatorio, esperamos que el conjunto de test tenga la misma distribución:

```{r}
# Separamos el dataframe en dos, uno con entradas de personas empleables y otro
# con entradas de personas no empleables. Contamos el numero de filas de cada
# sub-dataframe
employables <- nrow(df_train[df_train$CLASS == "Empleable", ])
non_employables <- nrow(df_train[df_train$CLASS == "No empleable", ])

message("TRAINING:\nEmpleables: ", employables, "    No empleables: ", non_employables)
```

Es más fácil de interpretar si vemos los porcentajes:

```{r}
total <- employables + non_employables
employables <- employables / total * 100
non_employables <- non_employables / total * 100

message("TRAINING:\nEmpleables: ", employables, "%    No empleables: ", non_employables, "%")
```

Veamos esto gráficamente:

```{r}
barplot(prop.table(table(df_train$CLASS)),
        col = rainbow(2),
        ylim = c(0, 0.7),
        main = "Distribución de la empleabilidad en los datos de entrenamient")
```

Hay cierto desbalanceo hacia la no empleabilidad, aproximadamente un 40-60%. De normal, este desbalanceo no es demasiado grave. Además, considerando el desbalanceo esperable dado el contexto en el que estamos (un porcentaje mayoritario no debería ser aceptado para un trabajo), consideramos que no necesitamos aplicar alguna técnica para tratar este desbalanceo. Aunque no es algo demasiado complicado de aplicar, por ejemplo, [SMOTE](https://www.statology.org/smote-in-r/)

## Estudio del supuesto de normalidad

----------------------------------------------------------------------------------------------------

# Análisis exploratorio multivariante

## Comprobar supuestos de correlación entre variables

Barlett

## Outliers multivariantes

## Gráficos por parejas de variables de entrada

## Tratar valores perdidos si no los hemos tratado ya

## Reducción de la dimensionalidad con componentes principales o variables latentes

## Estudiar el supuesto de normalidad multivariante

----------------------------------------------------------------------------------------------------

# Clasificación

## Clasificador con discriminante lineal

## Clasificador con discriminante cuadrático

----------------------------------------------------------------------------------------------------

# Validación

## Validación con matriz de confusión






<!-- Al final, siempre tenemos las notas a pie de página -->
<!-- Así que ponemos un título para que está sección se vea correctamente -->

# Notas a pie de página
