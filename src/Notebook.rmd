---
title: Estudio sobre la empleabilidad de los estudiantes
author:
    - Sergio Quijano Rey
    - sergioquijano@correo.ugr.es
date:
output:
    html_document:
        toc: true
        toc_depth: 3
        toc_float: true
        collapsed: true
        number_sections: true
---

# Introducción

Vamos a trabajar con una base de datos sobre la empleabilidad de ciertos estudiantes tras hacer una entrevista de trabajo, a modo de entrenamiento para su futura búsqueda de trabajo.

Tenemos dos objetivos en este trabajo:

1. Construir un modelo que prediga, en base a las métricas recogidas durante la entrevista, si una persona es empleable o no
2. Estudiar las métricas recogidas, buscando ver cuáles son las más relevantes <!-- TODO -- quizas esto es mentira por lo que voy a hacer mas tarde -->

Notar que todo el código, así como el historial del desarrollo de este, está disponible en [este repositorio de Github](https://github.com/SergioQuijanoRey/EstadisticaMultivariantePracticaFinal)

# Paquetes utilizados

A continuación, importamos todos los paquetes que vamos a utilizar:

```{r}
# Para leer archivos .sav
library(foreign)

# Para leer archivos excel
library(readxl)

# Para hacer ciertos gráficos
library(tidyr)
library(ggplot2)

# Para hacer "string interpolation"
library(stringr)

# Para visualizar la matriz de correlacion
library(ggcorrplot)

# Para hacer graficos de la matriz de correlacion pero visualizandola como un
# grafico de barras
library(lares)

# Para hacer el test de Bartlett
library(psych)

# Para normalizar datos
library(caret)

# Para mostrar algunas graficas sobre PCA
library("factoextra")
```

Estos paquetes se pueden instalar con la orden `install.packages("<nombre_paquete>")`. Sin embargo, hemos definido todos los paquetes necesarios, de forma declarativa, en el archivo `shell.nix` que se puede encontrar en [este archivo](https://github.com/SergioQuijanoRey/EstadisticaMultivariantePracticaFinal/blob/main/shell.nix). Para usar este entorno de paquetes se puede consultar la [documentación oficial de Nix](https://nixos.org/)

# Descripción de la base de datos

La base de datos se puede encontrar en el repositorio [Kaggle](https://www.kaggle.com/datasets/anashamoutni/students-employability-dataset). Este *dataset* contiene datos recogidos por agencias universitarias en las Filipinas. Contiene datos de entrevistas de trabajo de pruebas (para entrenar a los estudiantes en su futura búsqueda de trabajo). El *dataset* no ha sido limpiado ni procesado ^[Según lo que se dice el Kaggle, sin embargo más adelante comentamos que esto no es del todo cierto], así que este trabajo lo tendremos que realizar nosotros.

Este *dataset* se compone de 10 columnas y 2983 filas. Las columnas se corresponden con:

1. Identificador numérico del estudiante. Lo llaman nombre del estudiante, pero en verdad se corresponde con una etiqueta numérica del tipo "Student <id>"
2. Apariencia general
3. Formas de hablar
4. Condición física
5. Agilidad mental
6. Confianza en si mismo
7. Habilidad para presentar ideas
8. Habilidades de comunicación
9. Rendimiento académico
10. Empleabilidad

Todas estas variables, salvo el identificador y la empleabilidad, son valores discretos del 1 al 5. La estructura del identificador ya la hemos comentado. La empleabilidad es una variable de tipo texto con los valores `"Employable"` y `"LessEmployable"`. Esta es la variable que queremos predecir a partir de las métricas recogidas durante la entrevista.

Si nos fijamos en el número de filas y los identificadores de los estudiantes, podemos ver que tenemos hasta el identificador 3000, mientras que tenemos 22 filas menos. Esto nos hace pensar que sí que se han procesado los datos, eliminando 22 filas o bien con datos faltantes o bien con *outliers*.

Como veremos más adelante, el *dataset* no presenta datos faltantes, así que lo más seguro es que las filas eliminadas sean por los datos faltantes.

Con todo esto, ya estamos en condiciones de importar los datos:

```{r}
df <- read_excel("../data/Student-Employability-Datasets.xlsx")
head(df)
```

Y ahora, un resumen del *dataset*:

```{r}
summary(df)
```

A partir del resumen, podemos ver que:

- Aunque las métricas van, en un diseño inicial, del valor 1 al 5, muchas de ellas tienen un rango más acotado (por debajo), puesto que no hubo participantes que obtuviesen los valores mínimos.
- Todas las variables tienen, como valor mínimo, el 2. Solo la variable `Student Performance Rating` tuvo un valor mínimo de 3


# Análisis Exploratorio Univariante

## Procesamiento inicial de los datos

Lo primero que tenemos que hacer es eliminar la primera fila del *dataset*. Esta se corresponde con un identificador de los estudiantes, que no nos va a ser útil para nada:

```{r}
df <- df[, -1]
head(df)
```

Por comodidad, cambiamos el nombre de las columnas de datos:

```{r}
colnames(df) <- c("APARIENCIA", "FORMAS HABLAR", "CONDICION FISICA", "AGILIDAD MENTAL", "CONFIANZA", "HAB. PRESENTAR IDEAS", "HABILIDADES COMUNICACION", "RENDIMIENTO ACADEMICO", "CLASS")
head(df)
```

## Separación en training / test

Para realizar una posterior validación cruzada, tenemos que separar los datos en dos conjuntos, uno de entrenamiento y otro de test. Además, todo el procesamiento de datos que realicemos, deberá ser de la siguiente forma, para evitar caer en *data snooping*: ^[Para un lector que no esté familiarizado con este concepto, se puede consultar [este recuros](https://web.ma.utexas.edu/users/mks/statmistakes/datasnooping.html)]

- Todo el procesamiento de datos debe aprenderse sobre el conjunto de entrenamiento
- Este procesamiento aprendido en el conjunto de entrenamiento debe aplicarse ciegamente sobre el conjunto de test
- Por ejemplo, si se decide estandarizar una variable, se debe aplicar usando la media y desviación del conjunto de entrenamiento. Ese valor, sobre el conjunto de entrenamiento, se aplica después en la normalización del conjunto de test, y no los estadísticos en test

Algunas operaciones, por su naturaleza, no tienen por qué ser tan estrictas. Por ejemplo, una recodificación de una variable de tipo texto en tipo numérico de forma sencilla (como hacemos en [Recodificaciones o agrupaciones de datos]).

Dicho esto, separamos en los dos conjuntos de datos. Hacemos un muestreo aleatorio porque en muchos conjuntos de datos, el orden de los datos involucra cierto patrón:

```{r}
# Estableciendo la semilla, hacemos que los resultados sean reproducibles
set.seed(123456789)

# Cuantos datos queremos en training
train_percentage <- 0.80
train_size <- floor(train_percentage * nrow(df))

# Muestreamos los indices de training
train_indixes <- sample(seq_len(nrow(df)), size = train_size)

# Con los indices, hacemos la separacion
df_train <- df[train_indixes, ]
df_test <- df[-train_indixes, ]

# Vemos los tamaños de los datasets obtenidos

message("Conjunto de entrenamiento")
message(ncol(df_train), " , ", nrow(df_train))

message("Conjunto de test")
message(ncol(df_test), " , ", nrow(df_test))
```

## Recodificaciones o agrupaciones de datos

Por lo que hemos visto previamente, la variable que queremos predecir toma dos valores de tipo texto. Muchos de los métodos de clasificación trabajan (solo, o de forma mejor) con variables de tipo numérico. Así que realizamos el siguiente cambio:

- `Employable -> 1`
- `LessEmployable -> 0`

Además, haremos que la variable numérica sea un factor, porque ciertas funciones de `R` necesitan esto para trabajar de forma correcta.

```{r}
# Aplica un cambio de variable a la variable objetivo
# `Employable -> 1`
# `LessEmployable -> 0`
change_target_format <- function(target){
    if (length(target) > 1){
        stop("Se ha pasado un vector en vez de un unico valor, que es lo que espera la funcion")
    }


    if(target == "Employable"){
        return(1)
    } else if (target == "LessEmployable"){
        return(0)
    }

    # Esta situacion nunca deberia pasar
    # Tenemos una variable que no toma ninguno de los dos valores supuestos
    stop("La variable pasada no es ni 'Employable' ni 'LessEmployable'")
}

# Aplicamos dicha funcion a la salida
df_train$CLASS <- sapply(df_train$CLASS, change_target_format)

# Aplicamos un factor a la variable de salida
df_train <- within(df_train, {
    CLASS <- factor(CLASS, labels = c("Empleable", "No empleable"))
})

# Aplicamos el mismo procesamiento al dataframe de test
df_test$CLASS <- sapply(df_test$CLASS, change_target_format)

# Aplicamos un factor a la variable de salida
df_test <- within(df_test, {
    CLASS <- factor(CLASS, labels = c("Empleable", "No empleable"))
})
```

Veamos algunos datos de entrenamiento, ahora que hemos hecho la modificación:

```{r}
# Mostramos algunas entradas de los datos tras haber hecho la recodificacion de
# la variable de salida
head(df_train)
```

Y con el resumen del dataframe podemos ver que el cambio ha sido efectivo, a nivel del tipo de datos que estamos manejando:

```{r}
summary(df_train)
```

El resto de variables son numéricas discretas, que toman un rango de valores muy pequeño, y por lo tanto no parece necesario que las agrupemos.

## Valores perdidos

Como ya hemos comentado en [Descripción de la base de datos], sospechamos que el autor de la base de datos ya ha realizado la limpieza de los datos faltantes. A continuación, nos aseguramos de que no haya datos faltantes, ni en entrenamiento ni en test:

```{r}
sapply(
    df_train,
    function(col) {
        sum(is.na(col))
    }
)

sapply(
    df_test,
    function(col) {
        sum(is.na(col))
    }
)
```

Efectivamente, no tenemos valores perdidos, ni en entrenamiento ni en test. Ahora, como suponemos, por lo comentado previamente, que el *dataset* original tenía 3000 entradas, y nos quedamos con 22 menos, tenemos un porcentaje de valores perdidos:

```{r}
missing_values_perc <- 22 / 3000 * 100
missing_values_perc
```

Esto es algo menos del 0.74%, por lo tanto, no es necesario analizar el patrón aleatorio de los valores perdidos. Además, el procesamiento de estos valores perdidos ya ha sido impuesto por el autor de la base de datos, que ha decidido eliminarlos. Esto no nos supone un gran problema puesto que tenemos un volumen de datos considerable

## Análisis Descriptivo Numérico Clásico

El primer paso es observar la información que nos da la función `summary`. Aunque ya la hemos visto muchas veces, volvemos a consultarla:

```{r}
summary(df_train)
```

Veamos también los gráficos de cajas de cada una de las variables. Con esto, será más sencillo comentar los resultados que nos da la función `summary`, que nos da información sobre cuartiles, mínimos, máximos, mediana y media:

```{r}
boxplot(
    df_train,

    # Para que los nombres de las variables salgan en vertical
    las = 2,

    # Doy nombres acortados para que se vea mejor
    names = c("APARIENCIA", "FORM.HABLAR", "COND.FIS", "AGIL.MENTAL", "CONFIANZA", "PRESENTAR IDEAS", "COMUNICACION", "ACADEMICO", "CLASS")
)
```

A partir de esto, podemos ver que:

1. Como ya hemos comentado en [Descripción de la base de datos], ninguna de las variables toma el valor 1, aunque el diseño del estudio se hubiese hecho teniendo esto en mente, y el rendimiento académico tiene como valor mínimo el valor 3
2. Casi todas las variables tienen valores altos (en la escala $1-5$). Son destacables las variables apariencia física y rendimiento académico
    - Es de esperar que la mayoría de personas acudan a la entrevista de trabajo con una buena apariencia
    - Al ser un estudio realizado sobre personas universitarias, es de esperar que su rendimiento sea alto. Además, son universitarios que en un futuro cercano van a estar buscando trabajo, y por tanto estarán a punto de terminar sus estudios universitarios, lo que puede hacer que el rendimiento sea aún más sesgado a lo alto
3. Las variables más variadas son la condición física, la agilidad mental y la confianza en uno mismo
4. La habilidad comunicativa es aquella con mediana más baja (sin contar la variable de salida, que tenemos que considerar por separado)
5. Al estar trabajando con variables discretas, no podemos obtener una información tan fina con esta información como sí podríamos obtener con variables continuas. Además, las variables están en los mismos rangos $1-5$, así que estamos obligados a usar otros recursos para obtener información, previo al ajuste de un modelo, a partir de estos datos
6. Aunque lo veremos más adelante en [Balanceo de las clases], podemos ver que la clase "No empleable" es la predominante

Veamos ahora los histogramas de las variables:

```{r}
# Esta grafica la copiamos, sin apenas modificaciones, de:
# https://statisticsglobe.com/histogram-density-for-each-column-data-frame-r
data_long <- df_train[, -9] %>%
    pivot_longer(colnames(df_train[, -9])) %>%
    as.data.frame()

ggp1 <- ggplot(data_long, aes(x = value)) +
    geom_histogram(bins = 10) +
    facet_wrap(~ name, scales = "free")

ggp1
```

Con esta gráfica, podemos ver que:

1. Muchas de las variables (por ejemplo, agilidad mental, condición física, habilidad para presentar ideas, ...) parecen seguir una distribución normal. Esto lo comprobaremos más adelante con un contraste de hipótesis
2. Tanto el rendimiento académico, como las habilidades de comunicación, no son simétricas
    - En el caso de las habilidades de comunicación, con más concentración hacia la izquierda. Esto ya lo intuíamos al ver los gráficos de bigotes. La habilidad de comunicación es la que peor se les da a los candidatos, en relación a las otras variables
    - En el caso del rendimiento académico, casi todos los candidatos tienen una valoración de 5. La variable de la que podríamos esperar mayor relevancia a la hora de valorar la empleabilidad de un candidato, es de las que menos variabilidad presenta
3. Casi todas las variables se concentran en el rango $[3, 5]$, como ya sospechábamos previamente al ver un resumen rápido de los datos

## Outliers

En las gráficas de cajas anteriores, en [Análisis Descriptivo Numérico Clásico], hemos visto que la única variable que presenta *outliers* es la apariencia. Pero veamos esto de forma más clara usando el siguiente código (que se basa en el entregado en las *Entregas Voluntarias* de *PRADO*):

```{r}

# Devuelve el numero de outliers que tiene una columna de datos
how_many_outliers <- function(data) {
    H <- 1.5 * IQR(data)
    outliers <- which(
        data < quantile(data, 0.25, na.rm = TRUE) - H |
        data > quantile(data, 0.75, na.rm = TRUE) + H
    )
    return(length(outliers))
}

# Comprueba si una columna de datos tiene outliers
has_outliers <- function(data) {
    return(how_many_outliers(data) > 0)
}

# No miramos la variable de salida, porque no podemos calcular cuartiles sobre factores
# Ademas, no tiene sentido buscar valores atípicos en la salida que queremos predecir
sapply(df_train[, -9], has_outliers)
```

Vemos que, en efecto, solo la variable de la apariencia presenta *outliers*. Ahora, veamos cuántos *outliers* tenemos en dicha variable. Si son relativamente pocos, los eliminaremos. En otro caso, tendremos que sustituirlos por el valor de la media:

```{r}
no_outliers <- how_many_outliers(df_train$APARIENCIA)
perc_outliers <- no_outliers / length(df_train$APARIENCIA) * 100
message("Nº outliers: ", no_outliers, " --> ", perc_outliers, "%")
```

El porcentaje de filas con *outliers* está por debajo del 0.6%, y teniendo en cuenta el volumen de datos, procedemos a borrar aquellas filas con un *outlier* unidimensional en la variable de apariencia (de nuevo, usando un código que hemos adaptado del entregado en la *Entrega Voluntaria*). Aquí es crítico aplicar lo que hemos comentado previamente sobre *data snooping*.

```{r}

# Metricas, que calculamos sobre el conjunto de train, para aplicar el borrado
# de outliers
quantiles <- quantile(df_train$APARIENCIA, probs = c(0.25, 0.75), na.rm = FALSE)
H <- IQR(df_train$APARIENCIA) * 1.5
lower <- quantiles[1] - H
upper <- quantiles[2] + H

# Con estos estadisticos, borramos los outliers en train
rows_prev <- nrow(df_train)
df_train <- subset(df_train, df_train$APARIENCIA > lower & df_train$APARIENCIA < upper)
rows_deleted <- rows_prev - nrow(df_train)
message("TRAIN, hemos borrado ", rows_deleted, " filas, lo que supone un ", rows_deleted / rows_prev * 100, "%")

# Usando los mismos estadisticos, para evitar hacer data snooping, borramos
# outliers en test
rows_prev <- nrow(df_test)
df_test <- subset(df_test, df_test$APARIENCIA > lower & df_test$APARIENCIA < upper)
rows_deleted <- rows_prev - nrow(df_test)
message("TEST, hemos borrado ", rows_deleted, " filas, lo que supone un ", rows_deleted / rows_prev * 100, "%")
```

Ahora, podemos comprobar que no quedan *outliers* en el conjunto de entrenamiento:

```{r}
sapply(df_train[, -9], has_outliers)
```

Y en efecto, no tenemos ningún outlier. Veamos (aunque se pueda saber con los datos mostrados previamente), el tamaño de nuestros conjuntos de datos tras los borrados:

```{r}
nrow(df_train)
nrow(df_test)
```

Como era de esperar (pues hemos borrado porcentajes muy bajos de datos), seguimos teniendo conjuntos de datos grandes con los que trabajar.

## Balanceo de las clases

Esto se ha podido ver en operaciones realizadas anteriormente sobre la base de datos, pero lo hacemos ahora explícitamente. Solo lo hacemos para el conjunto de entrenamiento. Como hemos hecho un muestreo aleatorio, esperamos que el conjunto de test tenga la misma distribución:

```{r}
# Separamos el dataframe en dos, uno con entradas de personas empleables y otro
# con entradas de personas no empleables. Contamos el numero de filas de cada
# sub-dataframe
employables <- nrow(df_train[df_train$CLASS == "Empleable", ])
non_employables <- nrow(df_train[df_train$CLASS == "No empleable", ])

message("TRAINING:\nEmpleables: ", employables, "    No empleables: ", non_employables)
```

Es más fácil de interpretar si vemos los porcentajes:

```{r}
total <- employables + non_employables
employables <- employables / total * 100
non_employables <- non_employables / total * 100

message("TRAINING:\nEmpleables: ", employables, "%    No empleables: ", non_employables, "%")
```

Veamos esto gráficamente:

```{r}
barplot(prop.table(table(df_train$CLASS)),
        col = rainbow(2),
        ylim = c(0, 0.7),
        main = "Distribución de la empleabilidad en los datos de entrenamient")
```

Hay cierto desbalanceo hacia la no empleabilidad, aproximadamente un 40-60%. De normal, este desbalanceo no es demasiado grave. Además, considerando el desbalanceo esperable dado el contexto en el que estamos (un porcentaje mayoritario no debería ser aceptado para un trabajo), consideramos que no necesitamos aplicar alguna técnica para tratar este desbalanceo. Aunque no es algo demasiado complicado de aplicar, por ejemplo, [SMOTE](https://www.statology.org/smote-in-r/)

## Estudio del supuesto de normalidad

Posteriormente, vamos a usar métodos que dependen de la normalidad (multivariante, y por ende, univariante), así que en esta sección vamos a realizar un estudio sobre este supuesto.

### Exploración gráfica

Antes de plantear un contraste de hipótesis sobre el supuesto de normalidad, veamos primero algunos gráficos que nos van a dar ciertas intuiciones sobre la normalidad o falta de ella en nuestro conjunto de datos. Empecemos con los gráficos cuantil-cualtil o *Q-Q*. Al estar trabajando con variables discretas, con rangos muy pequeños, estos gráficos no nos van a dar demasiada información:

```{r}
# Para mostrar multiples graficos de forma mas compacta
par(mfrow=c(2,2))

# Iteramos sobre todas las variables del dataframe
for(name in names(df_train)){

    # No hacemos el qqplot de la variable de salida
    if(name == "CLASS"){
        next()
    }

    # Aplicamos el qqplot para la variable actual
    qqnorm(df_train[[name]], main = str_interp("QQPlot de la variable ${name}"))
    qqline(df_train[[name]])

}
```

Con estos gráficos podemos ver, cómo de centradas están las entradas según una normal, y cómo de dispersas están las variables para cada uno de los niveles (1, 2, $\ldots$, 5) respecto la normalidad. Con esto, podemos ver que:

1. La variable confianza parece que no sigue una normal, porque en el nivel `2` se desvía demasiado
2. Lo mismo ocurre con la condición física y con agilidad mental
3. A formas de hablar le ocurre parecido (falla en el nivel más bajo), pero la desviación no es tan grande como con las otras variables
4. Algunas variables sí que parecen coincidir correctamente, como por ejemplo el rendimiento académico o las habilidades de comunicación

### Test de hipótesis

Ahora que ya hemos hecho cierta exploración gráfica, pasemos a plantear un test de hipótesis. Sospechamos que ciertas variables sí van a pasar dicho test, pero otras no. Todo esto por lo que hemos comentado en [Exploración gráfica].

Usaremos el **test de Shapiro-Wilk**. La hipótesis nula $H_0$ es que nuestros datos (lo haremos para cada variable), siguen una distribución normal. Por tanto, buscamos no rechazar la hipótesis nula, y por tanto, *p-valores* por encima de $\alpha = 0.05$

```{r}
msg = ""
eps <- 0.05
for(name in names(df_train)){
    # No miramos la normalidad de la variable de salida
    if(name == "CLASS"){
        next()
    }

    # Tomamos el p-value del test
    p_value <- shapiro.test(df_train[[name]])$p.value

    # Construimos el mensaje para el usuario que lee los datos
    msg <- str_interp("${msg}\nVariable ${name} tiene p-value: ${p_value}")

    # Comprobamos si es normal o no, segun el test
    if(p_value > eps) {
        msg <- str_interp("${msg}  <- Es normal")
    } else {
        msg <- str_interp("${msg}  <- NO ES NORMAL!!!! ")
    }
}

# Mostramos los resultados al usuario
message(msg)
```

Claramente, no tenemos normalidad univariante, además, con un margen enorme. Así que posteriormente, cuando comprobemos la normalidad multivariante, tampoco la obtendremos.

Nuestra sospecha de que ciertas variables sí seguirían una normal no se sostiene frente a estos resultados.

Vistos los gráficos *qqplot*, podemos pensar que un problema fundamental a la hora de realizar estos tests es que las variables con las que estamos trabajando no son continuas. Además, son variables discretas que toman muy pocos valores (5 como máximo, 2 o 3 valores en la mayoría de casos).

Esto será un problema para aplicar ciertos métodos, como los del discriminante lineal o cuadrático. Tendremos que fijarnos más en el cuadrático, porque es más robusto frente a la falta de normalidad. Pero no sabemos si será capaz de funcionar bien con una falta tan grave de normalidad.

----------------------------------------------------------------------------------------------------

# Análisis Exploratorio Multivariante

## Comprobar supuestos de correlación entre variables

### Exploración gráfica

Empezamos viendo esto de forma gráfica. Además, para el estudio posterior que haremos sobre el problema, esta exploración nos será de gran utilidad. Empezamos mostrando una función auxiliar, para lidiar con que ciertas funciones gráficas no trabajan bien con factores:

```{r}
# Funcion que usamos para que el factor de la variable de salida se transforme a un numero normal. Ciertas funciones para hacer graficas requieren esto, y lo vamos a usar bastante
dont_use_factor <- function(df) {
    return(transform(df, CLASS = as.numeric(CLASS)))
}
```

Ahora, mostramos la matriz de correlaciones:

```{r}
corr_matrix <- cor(dont_use_factor(df_train))
ggcorrplot(corr_matrix)
```

Con esta matriz de correlaciones, podemos ver que:

1. No hay ninguna variable que esté altamente correlada con la variable que queremos predecir
2. La variable rendimiento académico no está apenas correlada con ninguna otra variable
    - Empezamos a pensar que, con todo lo que hemos visto, esta variable va a ser muy poco útil para predecir la empleabilidad de una persona
    - Es destacable pues, en una situación de plena meritocracia, esta variable pensamos que debería ser de las más importantes
3. El resto de variables sí que parecen estar correladas entre ellas. No somos capaces de ver grupos claros que podamos usar para una posterior reducción de dimensionalidad. Así que deberemos usar alguno de los métodos vistos en clase para elegir esta reducción

Ahora, usando un gráfico de barras, podemos visualizar cuáles son los pares de variables que más correladas están entre sí:

<!-- Eliminamos los mensajes de advertencia y otros logs al usar la funcion grafica -->
```{r, echo = FALSE, message = FALSE, warning = FALSE}
corr_cross(df_train, rm.na = TRUE, max_pvalue = 0.05, top = 15, grid = TRUE)
```

A partir de este gráfico podemos ver que:

1. La confianza está altamente correlada con, en primer lugar, la agilidad mental, y en segundo lugar, la condición física
2. En menor grado, la confianza está correlada con la habilidad para presentar ideas y habilidad de comunicación, prácticamente al mismo nivel
3. Aún en menor grado, la confianza está correlada con las formas de hablar
4. Parece que la variable **confianza** va a ser muy relevante. De los 6 pares de variables más correladas, 5 de esos pares involucran a la confianza

Usando la misma función, podemos ver correlaciones en función de si estamos trabajando con personas empleables o no empleables. Empezamos viendo los **empleables**:

```{r}
employables <- df_train[df_train$CLASS == "Empleable", ]

corr_cross(
    employables,
    rm.na = TRUE,
    max_pvalue = 0.05,
    top = 15,
    grid = TRUE,
)

```

Ahora veamos los **no empleables**:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
non_employables <- df_train[df_train$CLASS == "No empleable", ]
corr_cross(
    non_employables,
    rm.na = TRUE,
    max_pvalue = 0.05,
    top = 15,
    grid = TRUE,
)
```

Con esto, vemos que:

1. En los empleables, la apariencia está algo correlada inversamente con el rendimiento académico. Esto mismo ocurre en los no empleables. En ambos casos es una correlación negativa muy débil.
2. En ambos casos, y como ya hemos visto previamente, la confianza es la variable que más correlada está con otras variables, lo que nos hace sospechar que va a ser un concepto importante a la hora de realizar la reducción de la dimensionalidad
3. Lo mismo ocurre, en menor grado, con la apariencia, que está bastante correlada con otro grupo de variables
4. En el caso de los no empleables, confianza y apariencia están negativamente correladas (aunque de forma débil) con el rendimiento académico. Por ello sospechamos que:
    - El rendimiento académico no es decisivo a la hora de decidir sobre la empleabilidad
    - Es más relevante la apariencia y la confianza, que no están correladas de forma clara con el rendimiento académico. Es decir, una persona con un buen rendimiento depende de la confianza y apariencia. Y dos personas con el mismo rendimiento pueden tener dos confianzas / apariencias totalmente distintas, con cierto sesgo hacia unos valores confianza / apariencia bajos


### Contraste de hipótesis

Una vez hecha esta exploración gráfica, pasamos a comprobar el supuesto de correlación de variables con un test de hipótesis. Por la exploración previa pensamos que dicho test va a pasar sin problemas.

Usaremos el *test de esfericidad de Bartlett*. Con dicho *test* buscamos ver que las variables están correlacionadas de alguna forma. Para ello, la hipótesis nula $H_0$ es que la matriz de correlaciones es la identidad. Es decir, que las variables no están correlacionadas. Para ello en realidad busca ver que $det(corr_{matrix}) = 1$.

```{r}
# Para la matriz de correlacion, usamos valores numericos en vez de factores
corr_matrix <- cor(dont_use_factor(df_train))

# Aplicamos el test sobre la matriz de correlaciones
cortest.bartlett(corr_matrix)
```

El *p-value* es prácticamente cero, por tanto $p-value << 0.05$. Con ello, rechazamos $H_0$ y podemos pensar que las variables están correlacionadas de forma significativa. Por tanto, podemos pensar que podemos realizar predicciones con estos datos y que puede ser útil una reducción de la dimensionalidad.

## Outliers multivariantes

Previamente ya hemos tratado los *outliers* de forma univariante. Aunque en el guión de las prácticas no se diga nada, vamos a tratar de localizar y borrar *outliers* multivariantes. Para ello, vamos a usar la **distancia Mahalanobis** de los puntos a su media. Sabemos que la distancia Mahalanobis sigue una distribución chi cuadrado con $k$ grados de libertad, con $k$ el número de variables con las que estamos trabajando. Podemos usar esto y un valor de $alpha = 0.001$ para poner un punto de corte y borrar *outliers* según este criterio^[Esto lo tomo del siguiente [recurso online](https://www.r-bloggers.com/2019/01/a-new-way-to-handle-multivariate-outliers/)]:

```{r}
# Parametro alpha con el que realizamos el corte
alpha <- .001

# Solo nos quedamos con las variables de entrada
# No consideramos la variable de salida
df_train_input <- df_train[, 1:8]

# Calculamos las distancias mahalanobis
md <- mahalanobis(df_train_input, center = colMeans(df_train_input), cov = cov(df_train_input))

# Aplicamos el punto de corte
cutoff <- (qchisq(p = 1 - alpha, df = ncol(df_train_input)))

# Tomamos los indices de los outliers
outliers_indixes <- which(md > cutoff)

# Borramos sobre todo el dataframe
# Tomamos metricas para saber cuantos datos borramos
rows_prev <- nrow(df_train)
df_train <- df_train[-outliers_indixes, ]

rows_after <- nrow(df_train)
deleted_rows <- rows_prev - rows_after

# Mostramos cuantas filas borramos
message("Hemos borrado ", deleted_rows, " filas, lo que supone un ", rows_deleted / rows_prev * 100, "%")
```

El borrado de filas es por debajo del 0.1%, así que no supone un porcentaje significativo y nuestro conjunto de datos sigue siendo lo suficientemente grande.

No lo aplicamos en *test* para no caer en *data snooping*. Para aplicar esta técnica sobre el conjunto de *test*, deberíamos volver a calcular ciertas métricas sobre dicho conjunto de *test*, así que consideramos que es una mejor opción no tratar los *outliers* multivariantes en *test*. Con esto, esperamos que los modelos aprendidos sobre el conjunto de entrenamiento generalicen correctamente, siendo robustos frente a casos extremos nunca vistos.


## Tratar valores perdidos si no los hemos tratado ya

En la sección [Valores Perdidos] ya hemos hablado sobre el tratamiento de valores perdidos. No hay nada que añadir al considerar el problema de forma multivariante, así que no añadimos nada y nos referimos a dicha sección para más información sobre el tratamiento.

## Re-escalado de los datos

Ahora que ya hemos tratado los *outliers*, tanto de forma univariante como multivariante, y previo a aplicar técnicas muy sensibles a variables con escalas distintas, aplicamos ahora un re-escalado de los datos.

Tenemos dos alternativas: escalar y estandarizar las variables. Como hemos visto que hemos tenido problemas con la normalidad de los datos, aplicamos **estandarización** para tratar de suavizar algo este problema. Esto es, restamos la media y dividimos por la desviación. De nuevo, para evitar **data snooping**, hacemos esto con la media y desviación computadas sobre el conjunto de entrenamiento, y lo aplicamos tanto en entrenamiento como en *test*:

```{r}
# Tomamos los parametros con los que vamos a normalizar del conjunto de
# entrenamiento. Ademas, solo normalizamos las variables de entrada, no la
# variable de salida
normalization_parameters <- preProcess(df_train[, 1:8])

# Aplicamos dichos parametros en entrenamiento y en test
df_train[, 1:8] <- predict(normalization_parameters, df_train[, 1:8])
df_test[, 1:8] <- predict(normalization_parameters, df_test[, 1:8])

# Veamos las primeras entradas en entrenamiento
head(df_train)
```

Ahora podemos ver que tenemos media cero y desviación uno:

```{r}
# Medias de las variables de entrada
sapply(df_train[, 1:8], mean)

# Desviaciones de las variables de entrada
sapply(df_train[, 1:8], sd)
```

## Reducción de la dimensionalidad con componentes principales

### ¿Tiene sentido aplicar PCA?

En [Comprobar supuestos de correlación entre variables] ya hemos comprobado que las variables no son independientes. Por tanto, tiene sentido aplicar esta técnica.

Además, ya hemos realizado un tratamiento de los *outliers*, tanto a nivel univariante como a nivel multivariante.

### Aplicación de PCA

Aplicamos dicha técnica y, más adelante, en [Elección del número de componentes principales], elegiremos el número de componentes con las que nos quedamos. Además, ahora solo lo aplicamos sobre el conjunto de entrenamiento. Cuando hayamos realizado una exploración suficiente, aplicaremos el PCA con los parámetros aprendidos sobre el entrenamiento al conjunto de *test*, para evitar *data snooping*.

```{r}
# Aplicamos PCA sin considerar la columna de salida
#
# Esto nos devuelve un objeto con el que vamos a realizar distintos analisis,
# pero todavia no hemos modificado el dataframe para usar una transformacion
# concreta
df_train_pca_transf <- prcomp(df_train[, 1:8], scale = TRUE, center = TRUE)
```

Veamos el resumen de la técnica aplicada:

```{r}
summary(df_train_pca_transf)
```

Y para poder analizar esto de forma más intuitiva, mostramos los dos siguientes gráficos:

```{r}
# Este codigo esta copiado por completo del codigo de practicas visto en clase.
# Hacemos solo unas ligeras modificaciones sobre el nombre del dataframe que
# estamos usando

# Proporción de varianza explicada
varianza_explicada <- df_train_pca_transf$sdev^2 / sum(df_train_pca_transf$sdev^2)
ggplot(data = data.frame(varianza_explicada, pc = 1:8),
       aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
  labs(x = "Componente principal", y= " Proporción de varianza explicada")
```

```{r}
# Este codigo esta copiado por completo del codigo de practicas visto en clase.
# Hacemos solo unas ligeras modificaciones sobre el nombre del dataframe que
# estamos usando
# Proporción de varianza explicada acumulada

varianza_acum<-cumsum(varianza_explicada)
ggplot( data = data.frame(varianza_acum, pc = 1:8),
        aes(x = pc, y = varianza_acum ,fill=varianza_acum )) +
  geom_col(width = 0.5) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Proporción varianza explicada acumulada")
```

Con esto, podemos pensar que:

1. Con una o dos componentes principales tenemos la mayoría de la varianza explicada
2. Aunque en el gráfico de varianza acumulada podemos ver que las últimas 6 componentes principales explican prácticamente la misma varianza, que supone una acumulación significativa

### Elección del número de componentes principales


**Primer método**, **Regla de Abdi et al. (2010)**: Se promedia las varianzas explicadas por la componentes principales y se selecciona aquellas cuya proporción de varianza explicada supera la media ^[Esto lo tomo directamente del código visto en las práticas]:

```{r}
which(df_train_pca_transf$sdev^2 > mean(df_train_pca_transf$sdev^2))
```

Vemos que con este método, basta con tomar **las dos primeras componentes principales**.

**Segundo método**, buscar una varianza explicada mínima. En este caso, buscamos explicar como mínimo el 80% de la varianza:

```{r}
threshold <- 0.8
which(varianza_acum >= threshold)
```

En este caso, vemos que a partir de usar cuatro componentes principales, explicamos al menos el 80% de la varianza, y por tanto, deberíamos usar **las cuatro primeras componentes principales**.

**Tercer método**: **método del codo** apoyándonos en el siguiente gráfico:

```{r}
# Usamos el dataframe original, no el objeto creado por la funcion que aplica PCA
scree(df_train[, 1:8])
```

Vemos que para PCA, el número de componentes más alto para el que el gráfico queda por encima de uno es **dos componentes principales**.

**Cuarto método**: **análisis paralelo** apoyándonos en el siguiente gráfico:

```{r}

# Para evitar que se quede computando indefinidamente, indicamos que queremos
# usar solo un core del ordenador
options(mc.cores = 1)

# Con esto, ya podemos lanzar la funcion sin ningún problema
fa.parallel(
    # De nuevo, usamos el dataframe original
    df_train[, 1:8],

    # Para usar este metodo en PCA y no en analisis factorial
    fa = "pc",
    fm = "minres"
)
```

De nuevo, esto nos indica que debemos usar **dos componentes principales**.

**Conclusión**: aunque no expliquemos al menos el 80% de la variabilidad, 3 de los 4 métodos nos han indicado que debemos usar dos componentes principales

> Notar que hemos explicado, en la tarea de entrega obligatoria, qué son estos métodos y cómo se interpretan

### Aplicación de la técnica con el número de componentes escogido

Ahora que hemos decidido usar dos componentes principales, aplicamos dicha técnica, como ya hemos comentado, sin caer en realizar *data snooping*:

```{r}
# Calculamos la transformacion sobre el conjunto de entrenamiento
# Usamos `rank = 2` para usar solo dos componentes principales
df_train_pca_transf <- prcomp(
    df_train[, 1:8],
    scale = TRUE,
    center = TRUE,
    rank = 2,
)

# La aplicamos para generar un nuevo conjunto de entrenamiento
# Creamos un nuevo dataframe para preservar los datos originales
df_train_pca <- predict(df_train_pca_transf, newdata = df_train)

# Añadimos la variables de salida, que no ha participado en todo el proceso del
# estudio PCA
df_train_pca <- cbind(df_train_pca, df_train$CLASS)

# Añadimos los nombres adecuados
colnames(df_train_pca) <- c("PC1", "PC2", "CLASS")

# Mostramos algunas entradas del nuevo dataframe
head(df_train_pca)
```

Hacemos lo mismo para el conjunto de test. Usamos los parametros aprendidos sobre entrenamiento para evitar hacer *data snooping*:


```{r}
df_test_pca <- predict(df_train_pca_transf, newdata = df_test)
df_test_pca <- cbind(df_test_pca, df_test$CLASS)
colnames(df_test_pca) <- c("PC1", "PC2", "CLASS")
```

### Exploración de los resultados

En primer lugar, podemos ver gráficamente los nuevos datos, pues solo tenemos dos componentes principales:

```{r}
plot(df_train_pca[, 1], df_train_pca[, 2], col = c("red", "blue")[df_train_pca[, 3]])
```

*PCA* no ha conseguido separar claramente las dos clases. Esto es normal, puesto que el objetivo y el planteamiento de la técnica no es este. Viendo esto, parece que un discriminante lineal o cuadrático no va a ser suficiente para clasificar correctamente estos datos, y quizás deberemos usar algún modelo más avanzado de clasificación.

Ahora, veamos la relevancia de las variables en cada una de las componentes principales:

```{r}
fviz_pca_var(
    # Esta funcion actua sobre el objeto devuelto por la funcion prcomp
    df_train_pca_transf,

    repel=TRUE,
    col.var="cos2",
    legend.title="Distancia"
)+theme_bw()
```

Con esto, vemos que:

1. La segunda dimensión está compuesta prácticamente del rendimiento académico. Esto supone un 14% de la variabilidad explicada.
2. La primera dimensión está compuesta por el resto de variables. Con mayor fuerza, destacan la agilidad mental, condición física, habilidades de comunicación y confianza. Con menor relevancia, tenemos la habilidad para presentar ideas y la apariencia
3. Esto parece confirmar lo que hemos comentado previamente: el rendimiento académico tiene muy poco que ver con las otras variables, que tienen una mayor relevancia

Veamos ahora cada uno de los individuos según esta transformación:

```{r}
fviz_pca_ind(
    # Esta funcion actua sobre el objeto devuelto por la funcion prcomp
    df_train_pca_transf,
    col.ind = "contrib",
    gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
    repel=TRUE,
    legend.title="Contrib.var"
)+theme_bw()
```

## Reducción de la dimensionalidad con variables latentes (análisis factorial)

### ¿Tiene sentido el Análisis Factorial?

En [Comprobar supuestos de correlación entre variables] ya hemos comprobado que las variables no son independientes. Por tanto, tiene sentido aplicar esta técnica.

Además, ya hemos realizado un tratamiento de los *outliers*, tanto a nivel univariante como a nivel multivariante.

### Elección del número de componentes principales

**Primer método**: **método del codo**:

```{r}
scree(df_train[, 1:8])
```

Según este método, el número óptimo de **factores latentes es uno**.

**Segundo método**: **análisis paralelo**:

```{r}
# Para evitar que se quede computando indefinidamente, indicamos que queremos
# usar solo un core del ordenador
options(mc.cores = 1)

# Con esto, ya podemos lanzar la funcion sin ningún problema
fa.parallel(
    df_train[, 1:8],
    n.obs = 200,
    fa = "fa",
    fm = "minres"
)
```

Visualmente, este método también nos indica que el número óptimo de **factores latentes es dos**. Sin embargo, la función nos dice por un mensaje que el **número de factores sea dos**

**Conclusiones**:

1. Usaremos un único factor latente, aunque podríamos usar dos factores latentes
2. Esto coincide en parte con la información que hemos obtenido de aplicar *PCA*
    - Seguramente tengamos una variable que es combinación de todas salvo el rendimiento académico
    - Y seguramente si consideramos dos factores latentes, el segundo sea el asociado con el rendimiento académico

### Aplicación de un modelo concreto

Ahora, calculamos un modelo con dos factores latentes, para comprobar las sospechas a raíz de la exploración anterior. De nuevo, hay que tener cuidado con no caer en *data snooping*. Usamos directamente máxima verosimilitud, porque en las clases de prácticas hemos visto que nos han dado mejores resultados.

```{r}
modelo_varimax <- fa(
    df_train[, 1:8],
    nfactors = 2,
    rotate = "varimax",
    fa="mle"
)
```

Mostramos la matriz de pesos factorial:

```{r}
print(modelo_varimax$loadings,cut=0)
```

Veamos esto de una forma mucho más sencilla de interpretar:

```{r}
fa.diagram(modelo_varimax)
```

Esta gráfica **confirma completamente nuestras sospechas**: tenemos una variable latente que combina todas aquellas variables que no son el rendimiento académico, y una segunda variable latente para representar el rendimiento académico. Además, por todo lo que hemos visto, el rendimiento académico es menos relevante que las otras variables, y por tanto, **podemos pensar que esta variable latente es menos relevante que la otra**.

<!-- TODO -- mirar si es suficiente con una variable latente -->

## Estudiar el supuesto de normalidad multivariante


----------------------------------------------------------------------------------------------------

# Clasificación

## Clasificador con discriminante lineal

## Clasificador con discriminante cuadrático

----------------------------------------------------------------------------------------------------

# Validación

## Validación con matriz de confusión






<!-- Al final, siempre tenemos las notas a pie de página -->
<!-- Así que ponemos un título para que está sección se vea correctamente -->

# Notas a pie de página
