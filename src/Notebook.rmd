---
title: Estudio sobre la empleabilidad de los estudiantes
author:
    - Sergio Quijano Rey
    - sergioquijano@correo.ugr.es
date:
output:
    html_document:
        toc: true
        toc_depth: 3
        toc_float: true
        collapsed: true
        number_sections: true
---

# Introducción

Vamos a trabajar con una base de datos sobre la empleabilidad de ciertos estudiantes tras hacer una entrevista de trabajo, a modo de entrenamiento para su futura búsqueda de trabajo.

Tenemos dos objetivos en este trabajo:

1. Construir un modelo que prediga, en base a las métricas recogidas durante la entrevista, si una persona es empleable o no
2. Estudiar las métricas recogidas, buscando ver cuáles son las más relevantes <!-- TODO -- quizas esto es mentira por lo que voy a hacer mas tarde -->

Notar que todo el código, así como el historial del desarrollo de este, está disponible en [este repositorio de Github](https://github.com/SergioQuijanoRey/EstadisticaMultivariantePracticaFinal)

# Paquetes utilizados

A continuación, importamos todos los paquetes que vamos a utilizar:

```{r}
# Para leer archivos .sav
library(foreign)

# Para leer archivos excel
library(readxl)

# Para hacer ciertos gráficos
library(tidyr)
library(ggplot2)

# Para hacer "string interpolation"
library(stringr)

# Para visualizar la matriz de correlacion
library(ggcorrplot)

# Para hacer graficos de la matriz de correlacion pero visualizandola como un
# grafico de barras
library(lares)

# Para hacer el test de Bartlett
library(psych)

# Para normalizar datos
# Tambien lo usamos para k-fold Cross Validation
# Tambien para la matriz de confusion
library(caret)

# Para mostrar algunas graficas sobre PCA
library(factoextra)

# Para hacer tests de normalidad
library(MVN)

# Para los tests de homogeneidad de la varianza
library(biotools)

# Para usar el modelo xgboost
library(xgboost)
```

Estos paquetes se pueden instalar con la orden `install.packages("<nombre_paquete>")`. Sin embargo, hemos definido todos los paquetes necesarios, de forma declarativa, en el archivo `shell.nix` que se puede encontrar en [este archivo](https://github.com/SergioQuijanoRey/EstadisticaMultivariantePracticaFinal/blob/main/shell.nix). Para usar este entorno de paquetes se puede consultar la [documentación oficial de Nix](https://nixos.org/)

# Descripción de la base de datos

La base de datos se puede encontrar en el repositorio [Kaggle](https://www.kaggle.com/datasets/anashamoutni/students-employability-dataset). Este *dataset* contiene datos recogidos por agencias universitarias en las Filipinas. Contiene datos de entrevistas de trabajo de pruebas (para entrenar a los estudiantes en su futura búsqueda de trabajo). El *dataset* no ha sido limpiado ni procesado ^[Según lo que se dice el Kaggle, sin embargo más adelante comentamos que esto no es del todo cierto], así que este trabajo lo tendremos que realizar nosotros.

Este *dataset* se compone de 10 columnas y 2983 filas. Las columnas se corresponden con:

1. Identificador numérico del estudiante. Lo llaman nombre del estudiante, pero en verdad se corresponde con una etiqueta numérica del tipo "Student <id>"
2. Apariencia general
3. Formas de hablar
4. Condición física
5. Agilidad mental
6. Confianza en si mismo
7. Habilidad para presentar ideas
8. Habilidades de comunicación
9. Rendimiento académico
10. Empleabilidad

Todas estas variables, salvo el identificador y la empleabilidad, son valores discretos del 1 al 5. La estructura del identificador ya la hemos comentado. La empleabilidad es una variable de tipo texto con los valores `"Employable"` y `"LessEmployable"`. Esta es la variable que queremos predecir a partir de las métricas recogidas durante la entrevista.

Si nos fijamos en el número de filas y los identificadores de los estudiantes, podemos ver que tenemos hasta el identificador 3000, mientras que tenemos 22 filas menos. Esto nos hace pensar que sí que se han procesado los datos, eliminando 22 filas o bien con datos faltantes o bien con *outliers*.

Como veremos más adelante, el *dataset* no presenta datos faltantes, así que lo más seguro es que las filas eliminadas sean por los datos faltantes.

Con todo esto, ya estamos en condiciones de importar los datos:

```{r}
df <- read_excel("../data/Student-Employability-Datasets.xlsx")
head(df)
```

Y ahora, un resumen del *dataset*:

```{r}
summary(df)
```

A partir del resumen, podemos ver que:

- Aunque las métricas van, en un diseño inicial, del valor 1 al 5, muchas de ellas tienen un rango más acotado (por debajo), puesto que no hubo participantes que obtuviesen los valores mínimos.
- Todas las variables tienen, como valor mínimo, el 2. Solo la variable `Student Performance Rating` tuvo un valor mínimo de 3


# Análisis Exploratorio Univariante

## Procesamiento inicial de los datos

Lo primero que tenemos que hacer es eliminar la primera fila del *dataset*. Esta se corresponde con un identificador de los estudiantes, que no nos va a ser útil para nada:

```{r}
df <- df[, -1]
head(df)
```

Por comodidad, cambiamos el nombre de las columnas de datos:

```{r}
colnames(df) <- c("APARIENCIA", "FORMAS HABLAR", "CONDICION FISICA", "AGILIDAD MENTAL", "CONFIANZA", "HAB. PRESENTAR_IDEAS", "HABILIDADES COMUNICACION", "RENDIMIENTO ACADEMICO", "CLASS")
head(df)
```

## Separación en training / test

Para realizar una posterior validación cruzada, tenemos que separar los datos en dos conjuntos, uno de entrenamiento y otro de test. Además, todo el procesamiento de datos que realicemos, deberá ser de la siguiente forma, para evitar caer en *data snooping*: ^[Para un lector que no esté familiarizado con este concepto, se puede consultar [este recuros](https://web.ma.utexas.edu/users/mks/statmistakes/datasnooping.html)]

- Todo el procesamiento de datos debe aprenderse sobre el conjunto de entrenamiento
- Este procesamiento aprendido en el conjunto de entrenamiento debe aplicarse ciegamente sobre el conjunto de test
- Por ejemplo, si se decide estandarizar una variable, se debe aplicar usando la media y desviación del conjunto de entrenamiento. Ese valor, sobre el conjunto de entrenamiento, se aplica después en la normalización del conjunto de test, y no los estadísticos en test

Algunas operaciones, por su naturaleza, no tienen por qué ser tan estrictas. Por ejemplo, una recodificación de una variable de tipo texto en tipo numérico de forma sencilla (como hacemos en [Recodificaciones o agrupaciones de datos]).

Dicho esto, separamos en los dos conjuntos de datos. Hacemos un muestreo aleatorio porque en muchos conjuntos de datos, el orden de los datos involucra cierto patrón:

```{r}
# Estableciendo la semilla, hacemos que los resultados sean reproducibles
set.seed(123456789)

# Cuantos datos queremos en training
train_percentage <- 0.80
train_size <- floor(train_percentage * nrow(df))

# Muestreamos los indices de training
train_indixes <- sample(seq_len(nrow(df)), size = train_size)

# Con los indices, hacemos la separacion
df_train <- df[train_indixes, ]
df_test <- df[-train_indixes, ]

# Vemos los tamaños de los datasets obtenidos

message("Conjunto de entrenamiento")
message(ncol(df_train), " , ", nrow(df_train))

message("Conjunto de test")
message(ncol(df_test), " , ", nrow(df_test))
```

## Recodificaciones o agrupaciones de datos

Por lo que hemos visto previamente, la variable que queremos predecir toma dos valores de tipo texto. Muchos de los métodos de clasificación trabajan (solo, o de forma mejor) con variables de tipo numérico. Así que realizamos el siguiente cambio:

- `Employable -> 1`
- `LessEmployable -> 0`

Además, haremos que la variable numérica sea un factor, porque ciertas funciones de `R` necesitan esto para trabajar de forma correcta.

```{r}
# Aplica un cambio de variable a la variable objetivo
# `Employable -> 1`
# `LessEmployable -> 0`
change_target_format <- function(target){
    if (length(target) > 1){
        stop("Se ha pasado un vector en vez de un unico valor, que es lo que espera la funcion")
    }


    if(target == "Employable"){
        return(1)
    } else if (target == "LessEmployable"){
        return(0)
    }

    # Esta situacion nunca deberia pasar
    # Tenemos una variable que no toma ninguno de los dos valores supuestos
    stop("La variable pasada no es ni 'Employable' ni 'LessEmployable'")
}

# Aplicamos dicha funcion a la salida
df_train$CLASS <- sapply(df_train$CLASS, change_target_format)

# Aplicamos un factor a la variable de salida
df_train <- within(df_train, {
    CLASS <- factor(CLASS, labels = c("Empleable", "No empleable"))
})

# Aplicamos el mismo procesamiento al dataframe de test
df_test$CLASS <- sapply(df_test$CLASS, change_target_format)

# Aplicamos un factor a la variable de salida
df_test <- within(df_test, {
    CLASS <- factor(CLASS, labels = c("Empleable", "No empleable"))
})
```

Veamos algunos datos de entrenamiento, ahora que hemos hecho la modificación:

```{r}
# Mostramos algunas entradas de los datos tras haber hecho la recodificacion de
# la variable de salida
head(df_train)
```

Y con el resumen del dataframe podemos ver que el cambio ha sido efectivo, a nivel del tipo de datos que estamos manejando:

```{r}
summary(df_train)
```

El resto de variables son numéricas discretas, que toman un rango de valores muy pequeño, y por lo tanto no parece necesario que las agrupemos.

## Valores perdidos

Como ya hemos comentado en [Descripción de la base de datos], sospechamos que el autor de la base de datos ya ha realizado la limpieza de los datos faltantes. A continuación, nos aseguramos de que no haya datos faltantes, ni en entrenamiento ni en test:

```{r}
sapply(
    df_train,
    function(col) {
        sum(is.na(col))
    }
)

sapply(
    df_test,
    function(col) {
        sum(is.na(col))
    }
)
```

Efectivamente, no tenemos valores perdidos, ni en entrenamiento ni en test. Ahora, como suponemos, por lo comentado previamente, que el *dataset* original tenía 3000 entradas, y nos quedamos con 22 menos, tenemos un porcentaje de valores perdidos:

```{r}
missing_values_perc <- 22 / 3000 * 100
missing_values_perc
```

Esto es algo menos del 0.74%, por lo tanto, no es necesario analizar el patrón aleatorio de los valores perdidos. Además, el procesamiento de estos valores perdidos ya ha sido impuesto por el autor de la base de datos, que ha decidido eliminarlos. Esto no nos supone un gran problema puesto que tenemos un volumen de datos considerable

## Análisis Descriptivo Numérico Clásico

El primer paso es observar la información que nos da la función `summary`. Aunque ya la hemos visto muchas veces, volvemos a consultarla:

```{r}
summary(df_train)
```

Veamos también los gráficos de cajas de cada una de las variables. Con esto, será más sencillo comentar los resultados que nos da la función `summary`, que nos da información sobre cuartiles, mínimos, máximos, mediana y media:

```{r}
boxplot(
    df_train,

    # Para que los nombres de las variables salgan en vertical
    las = 2,

    # Doy nombres acortados para que se vea mejor
    names = c("APARIENCIA", "FORM_HABLAR", "COND_FIS", "AGIL_MENTAL", "CONFIANZA", "PRESENTAR_IDEAS", "COMUNICACION", "ACADEMICO", "CLASS")
)
```

A partir de esto, podemos ver que:

1. Como ya hemos comentado en [Descripción de la base de datos], ninguna de las variables toma el valor 1, aunque el diseño del estudio se hubiese hecho teniendo esto en mente, y el rendimiento académico tiene como valor mínimo el valor 3
2. Casi todas las variables tienen valores altos (en la escala $1-5$). Son destacables las variables apariencia física y rendimiento académico
    - Es de esperar que la mayoría de personas acudan a la entrevista de trabajo con una buena apariencia
    - Al ser un estudio realizado sobre personas universitarias, es de esperar que su rendimiento sea alto. Además, son universitarios que en un futuro cercano van a estar buscando trabajo, y por tanto estarán a punto de terminar sus estudios universitarios, lo que puede hacer que el rendimiento sea aún más sesgado a lo alto
3. Las variables más variadas son la condición física, la agilidad mental y la confianza en uno mismo
4. La habilidad comunicativa es aquella con mediana más baja (sin contar la variable de salida, que tenemos que considerar por separado)
5. Al estar trabajando con variables discretas, no podemos obtener una información tan fina con esta información como sí podríamos obtener con variables continuas. Además, las variables están en los mismos rangos $1-5$, así que estamos obligados a usar otros recursos para obtener información, previo al ajuste de un modelo, a partir de estos datos
6. Aunque lo veremos más adelante en [Balanceo de las clases], podemos ver que la clase "No empleable" es la predominante

Veamos ahora los histogramas de las variables:

```{r}
# Esta grafica la copiamos, sin apenas modificaciones, de:
# https://statisticsglobe.com/histogram-density-for-each-column-data-frame-r
data_long <- df_train[, -9] %>%
    pivot_longer(colnames(df_train[, -9])) %>%
    as.data.frame()

ggp1 <- ggplot(data_long, aes(x = value)) +
    geom_histogram(bins = 10) +
    facet_wrap(~ name, scales = "free")

ggp1
```

Con esta gráfica, podemos ver que:

1. Muchas de las variables (por ejemplo, agilidad mental, condición física, habilidad para presentar ideas, ...) parecen seguir una distribución normal. Esto lo comprobaremos más adelante con un contraste de hipótesis
2. Tanto el rendimiento académico, como las habilidades de comunicación, no son simétricas
    - En el caso de las habilidades de comunicación, con más concentración hacia la izquierda. Esto ya lo intuíamos al ver los gráficos de bigotes. La habilidad de comunicación es la que peor se les da a los candidatos, en relación a las otras variables
    - En el caso del rendimiento académico, casi todos los candidatos tienen una valoración de 5. La variable de la que podríamos esperar mayor relevancia a la hora de valorar la empleabilidad de un candidato, es de las que menos variabilidad presenta
3. Casi todas las variables se concentran en el rango $[3, 5]$, como ya sospechábamos previamente al ver un resumen rápido de los datos

## Outliers

En las gráficas de cajas anteriores, en [Análisis Descriptivo Numérico Clásico], hemos visto que la única variable que presenta *outliers* es la apariencia. Pero veamos esto de forma más clara usando el siguiente código (que se basa en el entregado en las *Entregas Voluntarias* de *PRADO*):

```{r}

# Devuelve el numero de outliers que tiene una columna de datos
how_many_outliers <- function(data) {
    H <- 1.5 * IQR(data)
    outliers <- which(
        data < quantile(data, 0.25, na.rm = TRUE) - H |
        data > quantile(data, 0.75, na.rm = TRUE) + H
    )
    return(length(outliers))
}

# Comprueba si una columna de datos tiene outliers
has_outliers <- function(data) {
    return(how_many_outliers(data) > 0)
}

# No miramos la variable de salida, porque no podemos calcular cuartiles sobre factores
# Ademas, no tiene sentido buscar valores atípicos en la salida que queremos predecir
sapply(df_train[, -9], has_outliers)
```

Vemos que, en efecto, solo la variable de la apariencia presenta *outliers*. Ahora, veamos cuántos *outliers* tenemos en dicha variable. Si son relativamente pocos, los eliminaremos. En otro caso, tendremos que sustituirlos por el valor de la media:

```{r}
no_outliers <- how_many_outliers(df_train$APARIENCIA)
perc_outliers <- no_outliers / length(df_train$APARIENCIA) * 100
message("Nº outliers: ", no_outliers, " --> ", perc_outliers, "%")
```

El porcentaje de filas con *outliers* está por debajo del 0.6%, y teniendo en cuenta el volumen de datos, procedemos a borrar aquellas filas con un *outlier* unidimensional en la variable de apariencia (de nuevo, usando un código que hemos adaptado del entregado en la *Entrega Voluntaria*). Aquí es crítico aplicar lo que hemos comentado previamente sobre *data snooping*.

```{r}

# Metricas, que calculamos sobre el conjunto de train, para aplicar el borrado
# de outliers
quantiles <- quantile(df_train$APARIENCIA, probs = c(0.25, 0.75), na.rm = FALSE)
H <- IQR(df_train$APARIENCIA) * 1.5
lower <- quantiles[1] - H
upper <- quantiles[2] + H

# Con estos estadisticos, borramos los outliers en train
rows_prev <- nrow(df_train)
df_train <- subset(df_train, df_train$APARIENCIA > lower & df_train$APARIENCIA < upper)
rows_deleted <- rows_prev - nrow(df_train)
message("TRAIN, hemos borrado ", rows_deleted, " filas, lo que supone un ", rows_deleted / rows_prev * 100, "%")

# Usando los mismos estadisticos, para evitar hacer data snooping, borramos
# outliers en test
rows_prev <- nrow(df_test)
df_test <- subset(df_test, df_test$APARIENCIA > lower & df_test$APARIENCIA < upper)
rows_deleted <- rows_prev - nrow(df_test)
message("TEST, hemos borrado ", rows_deleted, " filas, lo que supone un ", rows_deleted / rows_prev * 100, "%")
```

Ahora, podemos comprobar que no quedan *outliers* en el conjunto de entrenamiento:

```{r}
sapply(df_train[, -9], has_outliers)
```

Y en efecto, no tenemos ningún outlier. Veamos (aunque se pueda saber con los datos mostrados previamente), el tamaño de nuestros conjuntos de datos tras los borrados:

```{r}
nrow(df_train)
nrow(df_test)
```

Como era de esperar (pues hemos borrado porcentajes muy bajos de datos), seguimos teniendo conjuntos de datos grandes con los que trabajar.

## Balanceo de las clases

Esto se ha podido ver en operaciones realizadas anteriormente sobre la base de datos, pero lo hacemos ahora explícitamente. Solo lo hacemos para el conjunto de entrenamiento. Como hemos hecho un muestreo aleatorio, esperamos que el conjunto de test tenga la misma distribución:

```{r}
# Separamos el dataframe en dos, uno con entradas de personas empleables y otro
# con entradas de personas no empleables. Contamos el numero de filas de cada
# sub-dataframe
employables <- nrow(df_train[df_train$CLASS == "Empleable", ])
non_employables <- nrow(df_train[df_train$CLASS == "No empleable", ])

message("TRAINING:\nEmpleables: ", employables, "    No empleables: ", non_employables)
```

Es más fácil de interpretar si vemos los porcentajes:

```{r}
total <- employables + non_employables
employables <- employables / total * 100
non_employables <- non_employables / total * 100

message("TRAINING:\nEmpleables: ", employables, "%    No empleables: ", non_employables, "%")
```

Veamos esto gráficamente:

```{r}
barplot(prop.table(table(df_train$CLASS)),
        col = rainbow(2),
        ylim = c(0, 0.7),
        main = "Distribución de la empleabilidad en los datos de entrenamiento")
```

Hay cierto desbalanceo hacia la no empleabilidad, aproximadamente un 40-60%. De normal, este desbalanceo no es demasiado grave. Además, considerando el desbalanceo esperable dado el contexto en el que estamos (un porcentaje mayoritario no debería ser aceptado para un trabajo), consideramos que no necesitamos aplicar alguna técnica para tratar este desbalanceo. Aunque no es algo demasiado complicado de aplicar, por ejemplo, [SMOTE](https://www.statology.org/smote-in-r/)

## Estudio del supuesto de normalidad

Posteriormente, vamos a usar métodos que dependen de la normalidad (multivariante, y por ende, univariante), así que en esta sección vamos a realizar un estudio sobre este supuesto.

### Exploración gráfica

Antes de plantear un contraste de hipótesis sobre el supuesto de normalidad, veamos primero algunos gráficos que nos van a dar ciertas intuiciones sobre la normalidad o falta de ella en nuestro conjunto de datos. Empecemos con los gráficos cuantil-cualtil o *Q-Q*. Al estar trabajando con variables discretas, con rangos muy pequeños, estos gráficos no nos van a dar demasiada información:

```{r}
# Para mostrar multiples graficos de forma mas compacta
par(mfrow=c(2,2))

# Iteramos sobre todas las variables del dataframe
for(name in names(df_train)){

    # No hacemos el qqplot de la variable de salida
    if(name == "CLASS"){
        next()
    }

    # Aplicamos el qqplot para la variable actual
    qqnorm(df_train[[name]], main = str_interp("QQPlot de la variable ${name}"))
    qqline(df_train[[name]])

}
```

Con estos gráficos podemos ver, cómo de centradas están las entradas según una normal, y cómo de dispersas están las variables para cada uno de los niveles (1, 2, $\ldots$, 5) respecto la normalidad. Con esto, podemos ver que:

1. La variable confianza parece que no sigue una normal, porque en el nivel `2` se desvía demasiado
2. Lo mismo ocurre con la condición física y con agilidad mental
3. A formas de hablar le ocurre parecido (falla en el nivel más bajo), pero la desviación no es tan grande como con las otras variables
4. Algunas variables sí que parecen coincidir correctamente, como por ejemplo el rendimiento académico o las habilidades de comunicación

### Test de hipótesis

Ahora que ya hemos hecho cierta exploración gráfica, pasemos a plantear un test de hipótesis. Sospechamos que ciertas variables sí van a pasar dicho test, pero otras no. Todo esto por lo que hemos comentado en [Exploración gráfica].

Usaremos el **test de Shapiro-Wilk**. La hipótesis nula $H_0$ es que nuestros datos (lo haremos para cada variable), siguen una distribución normal. Por tanto, buscamos no rechazar la hipótesis nula, y por tanto, *p-valores* por encima de $\alpha = 0.05$

```{r}
msg = ""
eps <- 0.05
for(name in names(df_train)){
    # No miramos la normalidad de la variable de salida
    if(name == "CLASS"){
        next()
    }

    # Tomamos el p-value del test
    p_value <- shapiro.test(df_train[[name]])$p.value

    # Construimos el mensaje para el usuario que lee los datos
    msg <- str_interp("${msg}\nVariable ${name} tiene p-value: ${p_value}")

    # Comprobamos si es normal o no, segun el test
    if(p_value > eps) {
        msg <- str_interp("${msg}  <- Es normal")
    } else {
        msg <- str_interp("${msg}  <- NO ES NORMAL!!!! ")
    }
}

# Mostramos los resultados al usuario
message(msg)
```

Claramente, no tenemos normalidad univariante, además, con un margen enorme. Así que posteriormente, cuando comprobemos la normalidad multivariante, tampoco la obtendremos.

Nuestra sospecha de que ciertas variables sí seguirían una normal no se sostiene frente a estos resultados.

Vistos los gráficos *qqplot*, podemos pensar que un problema fundamental a la hora de realizar estos tests es que las variables con las que estamos trabajando no son continuas. Además, son variables discretas que toman muy pocos valores (5 como máximo, 2 o 3 valores en la mayoría de casos).

Esto será un problema para aplicar ciertos métodos, como los del discriminante lineal o cuadrático. Tendremos que fijarnos más en el cuadrático, porque es más robusto frente a la falta de normalidad. Pero no sabemos si será capaz de funcionar bien con una falta tan grave de normalidad.

----------------------------------------------------------------------------------------------------

# Análisis Exploratorio Multivariante

## Comprobar supuestos de correlación entre variables

### Exploración gráfica

Empezamos viendo esto de forma gráfica. Además, para el estudio posterior que haremos sobre el problema, esta exploración nos será de gran utilidad. Empezamos mostrando una función auxiliar, para lidiar con que ciertas funciones gráficas no trabajan bien con factores:

```{r}
# Funcion que usamos para que el factor de la variable de salida se transforme a un numero normal. Ciertas funciones para hacer graficas requieren esto, y lo vamos a usar bastante
dont_use_factor <- function(df) {
    return(transform(df, CLASS = as.numeric(CLASS)))
}
```

Ahora, mostramos la matriz de correlaciones:

```{r}
corr_matrix <- cor(dont_use_factor(df_train))
ggcorrplot(corr_matrix)
```

Con esta matriz de correlaciones, podemos ver que:

1. No hay ninguna variable que esté altamente correlada con la variable que queremos predecir
2. La variable rendimiento académico no está apenas correlada con ninguna otra variable
    - Empezamos a pensar que, con todo lo que hemos visto, esta variable va a ser muy poco útil para predecir la empleabilidad de una persona
    - Es destacable pues, en una situación de plena meritocracia, esta variable pensamos que debería ser de las más importantes
3. El resto de variables sí que parecen estar correladas entre ellas. Podemos ver que hay un bloque, el de todas las variables de entradad salvo el rendimiento académico, que están muy correladas entre sí. Luego tenemos el rendimiento que, como ya hemos comentado, no se correla con el resto de variables. Por tanto, al reducir la dimensionalidad, **podemos esperar**:
    - Obtener dos bloques, uno con el rendimiento académico solo y otro con el resto de variables de entrada combinadas de alguna forma
    - Un único bloque, descartando el rendimiento académico

Ahora, usando un gráfico de barras, podemos visualizar cuáles son los pares de variables que más correladas están entre sí:

<!-- Eliminamos los mensajes de advertencia y otros logs al usar la funcion grafica -->
```{r, echo = FALSE, message = FALSE, warning = FALSE}
corr_cross(df_train, rm.na = TRUE, max_pvalue = 0.05, top = 15, grid = TRUE)
```

A partir de este gráfico podemos ver que:

1. La confianza está altamente correlada con, en primer lugar, la agilidad mental, y en segundo lugar, la condición física
2. En menor grado, la confianza está correlada con la habilidad para presentar ideas y habilidad de comunicación, prácticamente al mismo nivel
3. Aún en menor grado, la confianza está correlada con las formas de hablar
4. Parece que la variable **confianza** va a ser muy relevante. De los 6 pares de variables más correladas, 5 de esos pares involucran a la confianza

Usando la misma función, podemos ver correlaciones en función de si estamos trabajando con personas empleables o no empleables. Empezamos viendo los **empleables**:

```{r}
employables <- df_train[df_train$CLASS == "Empleable", ]

corr_cross(
    employables,
    rm.na = TRUE,
    max_pvalue = 0.05,
    top = 15,
    grid = TRUE,
)

```

Ahora veamos los **no empleables**:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
non_employables <- df_train[df_train$CLASS == "No empleable", ]
corr_cross(
    non_employables,
    rm.na = TRUE,
    max_pvalue = 0.05,
    top = 15,
    grid = TRUE,
)
```

Con esto, vemos que:

1. En los empleables, la apariencia está algo correlada inversamente con el rendimiento académico. Esto mismo ocurre en los no empleables. En ambos casos es una correlación negativa muy débil.
2. En ambos casos, y como ya hemos visto previamente, la confianza es la variable que más correlada está con otras variables, lo que nos hace sospechar que va a ser un concepto importante a la hora de realizar la reducción de la dimensionalidad
3. Lo mismo ocurre, en menor grado, con la apariencia, que está bastante correlada con otro grupo de variables
4. En el caso de los no empleables, confianza y apariencia están negativamente correladas (aunque de forma débil) con el rendimiento académico. Por ello sospechamos que:
    - El rendimiento académico no es decisivo a la hora de decidir sobre la empleabilidad
    - Es más relevante la apariencia y la confianza, que no están correladas de forma clara con el rendimiento académico. Es decir, una persona con un buen rendimiento depende de la confianza y apariencia. Y dos personas con el mismo rendimiento pueden tener dos confianzas / apariencias totalmente distintas, con cierto sesgo hacia unos valores confianza / apariencia bajos


### Contraste de hipótesis

Una vez hecha esta exploración gráfica, pasamos a comprobar el supuesto de correlación de variables con un test de hipótesis. Por la exploración previa pensamos que dicho test va a pasar sin problemas.

Usaremos el *test de esfericidad de Bartlett*. Con dicho *test* buscamos ver que las variables están correlacionadas de alguna forma. Para ello, la hipótesis nula $H_0$ es que la matriz de correlaciones es la identidad. Es decir, que las variables no están correlacionadas. Para ello en realidad busca ver que $det(corr_{matrix}) = 1$.

```{r}
# Para la matriz de correlacion, usamos valores numericos en vez de factores
corr_matrix <- cor(dont_use_factor(df_train))

# Aplicamos el test sobre la matriz de correlaciones
cortest.bartlett(corr_matrix)
```

El *p-value* es prácticamente cero, por tanto $p-value << 0.05$. Con ello, rechazamos $H_0$ y podemos pensar que las variables están correlacionadas de forma significativa. Por tanto, podemos pensar que podemos realizar predicciones con estos datos y que puede ser útil una reducción de la dimensionalidad.

## Outliers multivariantes

Previamente ya hemos tratado los *outliers* de forma univariante. Aunque en el guión de las prácticas no se diga nada, vamos a tratar de localizar y borrar *outliers* multivariantes. Para ello, vamos a usar la **distancia Mahalanobis** de los puntos a su media. Sabemos que la distancia Mahalanobis sigue una distribución chi cuadrado con $k$ grados de libertad, con $k$ el número de variables con las que estamos trabajando. Podemos usar esto y un valor de $alpha = 0.001$ para poner un punto de corte y borrar *outliers* según este criterio^[Esto lo tomo del siguiente [recurso online](https://www.r-bloggers.com/2019/01/a-new-way-to-handle-multivariate-outliers/)]:

```{r}
# Parametro alpha con el que realizamos el corte
alpha <- .001

# Solo nos quedamos con las variables de entrada
# No consideramos la variable de salida
df_train_input <- df_train[, 1:8]

# Calculamos las distancias mahalanobis
md <- mahalanobis(df_train_input, center = colMeans(df_train_input), cov = cov(df_train_input))

# Aplicamos el punto de corte
cutoff <- (qchisq(p = 1 - alpha, df = ncol(df_train_input)))

# Tomamos los indices de los outliers
outliers_indixes <- which(md > cutoff)

# Borramos sobre todo el dataframe
# Tomamos metricas para saber cuantos datos borramos
rows_prev <- nrow(df_train)
df_train <- df_train[-outliers_indixes, ]

rows_after <- nrow(df_train)
deleted_rows <- rows_prev - rows_after

# Mostramos cuantas filas borramos
message("Hemos borrado ", deleted_rows, " filas, lo que supone un ", rows_deleted / rows_prev * 100, "%")
```

El borrado de filas es por debajo del 0.1%, así que no supone un porcentaje significativo y nuestro conjunto de datos sigue siendo lo suficientemente grande.

No lo aplicamos en *test* para no caer en *data snooping*. Para aplicar esta técnica sobre el conjunto de *test*, deberíamos volver a calcular ciertas métricas sobre dicho conjunto de *test*, así que consideramos que es una mejor opción no tratar los *outliers* multivariantes en *test*. Con esto, esperamos que los modelos aprendidos sobre el conjunto de entrenamiento generalicen correctamente, siendo robustos frente a casos extremos nunca vistos.


## Tratar valores perdidos si no los hemos tratado ya

En la sección [Valores Perdidos] ya hemos hablado sobre el tratamiento de valores perdidos. No hay nada que añadir al considerar el problema de forma multivariante, así que no añadimos nada y nos referimos a dicha sección para más información sobre el tratamiento.

## Re-escalado de los datos

Ahora que ya hemos tratado los *outliers*, tanto de forma univariante como multivariante, y previo a aplicar técnicas muy sensibles a variables con escalas distintas, aplicamos ahora un re-escalado de los datos.

Tenemos dos alternativas: escalar y estandarizar las variables. Como hemos visto que hemos tenido problemas con la normalidad de los datos, aplicamos **estandarización** para tratar de suavizar algo este problema. Esto es, restamos la media y dividimos por la desviación. De nuevo, para evitar **data snooping**, hacemos esto con la media y desviación computadas sobre el conjunto de entrenamiento, y lo aplicamos tanto en entrenamiento como en *test*:

```{r}
# Tomamos los parametros con los que vamos a normalizar del conjunto de
# entrenamiento. Ademas, solo normalizamos las variables de entrada, no la
# variable de salida
normalization_parameters <- preProcess(df_train[, 1:8])

# Aplicamos dichos parametros en entrenamiento y en test
df_train[, 1:8] <- predict(normalization_parameters, df_train[, 1:8])
df_test[, 1:8] <- predict(normalization_parameters, df_test[, 1:8])

# Veamos las primeras entradas en entrenamiento
head(df_train)
```

Ahora podemos ver que tenemos media cero y desviación uno:

```{r}
# Medias de las variables de entrada
sapply(df_train[, 1:8], mean)

# Desviaciones de las variables de entrada
sapply(df_train[, 1:8], sd)
```

## Pares de variables

Veamos ahora todos los pares de variables de entrada, y cómo clasifican la empleabilidad:

```{r}
pairs(
    x = df_train[, 1:8],
    col = c("firebrick", "green3")[dont_use_factor(df_train)$CLASS], pch = 19
)
```

Este gráfico puede no ser útil, porque suponemos que podemos tener *overlapping* en muchos pares de variables. Esto es, entradas en nuestro conjunto de datos que, teniendo mismos valores para un par de variables concreto, tienen una empleabilidad distinta.

Por tanto, no podemos usar este gráfico para extraer información relevante sobre los datos.

## Reducción de la dimensionalidad con componentes principales

### ¿Tiene sentido aplicar PCA?

En [Comprobar supuestos de correlación entre variables] ya hemos comprobado que las variables no son independientes. Por tanto, tiene sentido aplicar esta técnica.

Además, ya hemos realizado un tratamiento de los *outliers*, tanto a nivel univariante como a nivel multivariante.

### Aplicación de PCA

Aplicamos dicha técnica y, más adelante, en [Elección del número de componentes principales], elegiremos el número de componentes con las que nos quedamos. Además, ahora solo lo aplicamos sobre el conjunto de entrenamiento. Cuando hayamos realizado una exploración suficiente, aplicaremos el PCA con los parámetros aprendidos sobre el entrenamiento al conjunto de *test*, para evitar *data snooping*.

```{r}
# Aplicamos PCA sin considerar la columna de salida
#
# Esto nos devuelve un objeto con el que vamos a realizar distintos analisis,
# pero todavia no hemos modificado el dataframe para usar una transformacion
# concreta
df_train_pca_transf <- prcomp(df_train[, 1:8], scale = TRUE, center = TRUE)
```

Veamos el resumen de la técnica aplicada:

```{r}
summary(df_train_pca_transf)
```

Y para poder analizar esto de forma más intuitiva, mostramos los dos siguientes gráficos:

```{r}
# Este codigo esta copiado por completo del codigo de practicas visto en clase.
# Hacemos solo unas ligeras modificaciones sobre el nombre del dataframe que
# estamos usando

# Proporción de varianza explicada
varianza_explicada <- df_train_pca_transf$sdev^2 / sum(df_train_pca_transf$sdev^2)
ggplot(data = data.frame(varianza_explicada, pc = 1:8),
       aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
  labs(x = "Componente principal", y= " Proporción de varianza explicada")
```

```{r}
# Este codigo esta copiado por completo del codigo de practicas visto en clase.
# Hacemos solo unas ligeras modificaciones sobre el nombre del dataframe que
# estamos usando
# Proporción de varianza explicada acumulada

varianza_acum<-cumsum(varianza_explicada)
ggplot( data = data.frame(varianza_acum, pc = 1:8),
        aes(x = pc, y = varianza_acum ,fill=varianza_acum )) +
  geom_col(width = 0.5) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Proporción varianza explicada acumulada")
```

Con esto, podemos pensar que:

1. Con una o dos componentes principales tenemos la mayoría de la varianza explicada
2. Aunque en el gráfico de varianza acumulada podemos ver que las últimas 6 componentes principales explican prácticamente la misma varianza, que supone una acumulación significativa

### Elección del número de componentes principales


**Primer método**, **Regla de Abdi et al. (2010)**: Se promedia las varianzas explicadas por la componentes principales y se selecciona aquellas cuya proporción de varianza explicada supera la media ^[Esto lo tomo directamente del código visto en las práticas]:

```{r}
which(df_train_pca_transf$sdev^2 > mean(df_train_pca_transf$sdev^2))
```

Vemos que con este método, basta con tomar **las dos primeras componentes principales**.

**Segundo método**, buscar una varianza explicada mínima. En este caso, buscamos explicar como mínimo el 80% de la varianza:

```{r}
threshold <- 0.8
which(varianza_acum >= threshold)
```

En este caso, vemos que a partir de usar cuatro componentes principales, explicamos al menos el 80% de la varianza, y por tanto, deberíamos usar **las cuatro primeras componentes principales**.

**Tercer método**: **método del codo** apoyándonos en el siguiente gráfico:

```{r}
# Usamos el dataframe original, no el objeto creado por la funcion que aplica PCA
scree(df_train[, 1:8])
```

Vemos que para PCA, el número de componentes más alto para el que el gráfico queda por encima de uno es **dos componentes principales**.

**Cuarto método**: **análisis paralelo** apoyándonos en el siguiente gráfico:

```{r}

# Para evitar que se quede computando indefinidamente, indicamos que queremos
# usar solo un core del ordenador
options(mc.cores = 1)

# Con esto, ya podemos lanzar la funcion sin ningún problema
fa.parallel(
    # De nuevo, usamos el dataframe original
    df_train[, 1:8],

    # Para usar este metodo en PCA y no en analisis factorial
    fa = "pc",
    fm = "minres"
)
```

De nuevo, esto nos indica que debemos usar **dos componentes principales**.

**Conclusión**: aunque no expliquemos al menos el 80% de la variabilidad, 3 de los 4 métodos nos han indicado que debemos usar dos componentes principales

> Notar que hemos explicado, en la tarea de entrega obligatoria, qué son estos métodos y cómo se interpretan

### Aplicación de la técnica con el número de componentes escogido

Ahora que hemos decidido usar dos componentes principales, aplicamos dicha técnica, como ya hemos comentado, sin caer en realizar *data snooping*:

```{r}
# Calculamos la transformacion sobre el conjunto de entrenamiento
# Usamos `rank = 2` para usar solo dos componentes principales
df_train_pca_transf <- prcomp(
    df_train[, 1:8],
    scale = TRUE,
    center = TRUE,
    rank = 2,
)

# La aplicamos para generar un nuevo conjunto de entrenamiento
# Creamos un nuevo dataframe para preservar los datos originales
df_train_pca <- predict(df_train_pca_transf, newdata = df_train)

# Añadimos la variables de salida, que no ha participado en todo el proceso del
# estudio PCA
df_train_pca <- cbind(df_train_pca, df_train$CLASS)

# Añadimos los nombres adecuados
colnames(df_train_pca) <- c("PC1", "PC2", "CLASS")

# Mostramos algunas entradas del nuevo dataframe
head(df_train_pca)
```

Hacemos lo mismo para el conjunto de test. Usamos los parametros aprendidos sobre entrenamiento para evitar hacer *data snooping*:


```{r}
df_test_pca <- predict(df_train_pca_transf, newdata = df_test)
df_test_pca <- cbind(df_test_pca, df_test$CLASS)
colnames(df_test_pca) <- c("PC1", "PC2", "CLASS")
```

### Exploración de los resultados

En primer lugar, podemos ver gráficamente los nuevos datos, pues solo tenemos dos componentes principales:

```{r}
plot(df_train_pca[, 1], df_train_pca[, 2], col = c("red", "blue")[df_train_pca[, 3]])
```

*PCA* no ha conseguido separar claramente las dos clases. Esto es normal, puesto que el objetivo y el planteamiento de la técnica no es este. Viendo esto, parece que un discriminante lineal o cuadrático no va a ser suficiente para clasificar correctamente estos datos, y quizás deberemos usar algún modelo más avanzado de clasificación.

Ahora, veamos la relevancia de las variables en cada una de las componentes principales:

```{r}
fviz_pca_var(
    # Esta funcion actua sobre el objeto devuelto por la funcion prcomp
    df_train_pca_transf,

    repel=TRUE,
    col.var="cos2",
    legend.title="Distancia"
)+theme_bw()
```

Con esto, vemos que:

1. La segunda dimensión está compuesta prácticamente del rendimiento académico. Esto supone un 14% de la variabilidad explicada.
2. La primera dimensión está compuesta por el resto de variables. Con mayor fuerza, destacan la agilidad mental, condición física, habilidades de comunicación y confianza. Con menor relevancia, tenemos la habilidad para presentar ideas y la apariencia
3. Esto parece confirmar lo que hemos comentado previamente: el rendimiento académico tiene muy poco que ver con las otras variables, que tienen una mayor relevancia

Veamos ahora cada uno de los individuos según esta transformación:

```{r}
fviz_pca_ind(
    # Esta funcion actua sobre el objeto devuelto por la funcion prcomp
    df_train_pca_transf,
    col.ind = "contrib",
    gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
    repel=TRUE,
    legend.title="Contrib.var"
)+theme_bw()
```

## Reducción de la dimensionalidad con variables latentes (análisis factorial)

### ¿Tiene sentido el Análisis Factorial?

En [Comprobar supuestos de correlación entre variables] ya hemos comprobado que las variables no son independientes. Por tanto, tiene sentido aplicar esta técnica.

Además, ya hemos realizado un tratamiento de los *outliers*, tanto a nivel univariante como a nivel multivariante.

### Elección del número de componentes principales

**Primer método**: **método del codo**:

```{r}
scree(df_train[, 1:8])
```

Según este método, el número óptimo de **factores latentes es uno**.

**Segundo método**: **análisis paralelo**:

```{r}
# Para evitar que se quede computando indefinidamente, indicamos que queremos
# usar solo un core del ordenador
options(mc.cores = 1)

# Con esto, ya podemos lanzar la funcion sin ningún problema
fa.parallel(
    df_train[, 1:8],
    n.obs = 200,
    fa = "fa",
    fm = "minres"
)
```

Visualmente, este método también nos indica que el número óptimo de **factores latentes es dos**. Sin embargo, la función nos dice por un mensaje que el **número de factores sea dos**

**Conclusiones**:

1. Usaremos un único factor latente, aunque podríamos usar dos factores latentes
2. Esto coincide en parte con la información que hemos obtenido de aplicar *PCA*
    - Seguramente tengamos una variable que es combinación de todas salvo el rendimiento académico
    - Y seguramente si consideramos dos factores latentes, el segundo sea el asociado con el rendimiento académico

### Aplicación de un modelo concreto

Ahora, calculamos un modelo con dos factores latentes, para comprobar las sospechas a raíz de la exploración anterior. De nuevo, hay que tener cuidado con no caer en *data snooping*. Usamos directamente máxima verosimilitud, porque en las clases de prácticas hemos visto que nos han dado mejores resultados.

```{r}
df_train_fa_varimax <- fa(
    df_train[, 1:8],
    nfactors = 2,
    rotate = "varimax",
    fa="mle"
)
```

Mostramos la matriz de pesos factorial:

```{r}
print(df_train_fa_varimax$loadings, cut = 0)
```

Veamos esto de una forma mucho más sencilla de interpretar:

```{r}
fa.diagram(df_train_fa_varimax)
```

Esta gráfica **confirma completamente nuestras sospechas**: tenemos una variable latente que combina todas aquellas variables que no son el rendimiento académico, y una segunda variable latente para representar el rendimiento académico. Además, por todo lo que hemos visto, el rendimiento académico es menos relevante que las otras variables, y por tanto, **podemos pensar que esta variable latente es menos relevante que la otra**.

Veamos si dos factores son suficientes:

```{r}
factanal(df_train[, 1:8], factors = 2, rotation = "none")
```

El test nos dice que **con dos factores es suficiente**. Ahora, antes de plantearnos usar un modelo con un solo factor latente, veamos si es suficiente, con un test:

```{r}
factanal(df_train[, 1:8], factors = 1, rotation = "none")
```

Con un solo factor también es suficiente. Aunque sospechamos que, para realizar con cierto rendimiento la tarea de clasificación, una sola variable latente puede ser muy poco. Así que calculamos el modelo y lo mostramos, pero en otra variable. Así conservamos los dos modelos, con una y dos variables latentes:

```{r}
df_train_fa_varimax_one <- fa(
    df_train[, 1:8],
    nfactors = 1,
    rotate = "varimax",
    fa="mle"
)

fa.diagram(df_train_fa_varimax_one)
```

Es destacable que **el rendimiento académico parece no participar en la variable latente**, que combina el resto de variables. Por esto, pensamos que es mejor usar el modelo con dos variables latentes, porque tendremos más información a la hora de clasificar, y con dos variables no corremos el riesgo de caer en un *overfitting* por culpa de considerar variables latentes extras. Así que transformamos los dos conjuntos de datos, y de nuevo, con cuidado de no caer en *data snooping*. Empezamos transformando el conjunto de entrenamiento, que es más sencillo:

```{r}
# Tomamos los datos de las dos variables latentes
df_train_fa <- df_train_fa_varimax$scores

# Añadimos la variable de salida que no participa en el analisis factorial
df_train_fa <- cbind(df_train_fa, df_train$CLASS)

# Ahora, añadimos los nombres de las columnas
colnames(df_train_fa) <- c("MR1", "MR2", "CLASS")

# Mostramos algunas entradas
head(df_train_fa)
```

Aplicamos la misma transformación (con parámetros aprendidos sobre el entrenamiento) al conjunto de test. Para ello, tenemos que usar la función `predict.psych`. Se puede consultar [la documentación oficial de dicha función](https://www.rdocumentation.org/packages/psych/versions/2.2.5/topics/predict.psych):

```{r}
# Aplicamos la transformacion aprendida sobre el conjunto de entrenamiento
# Para ello, tenemos que usar el objeto que `fa` genera, el conjunto al que
# aplicamos la transformacion, y el conjunto de datos sobre el que se ha
# aprendido la transformacion
df_test_fa <- predict.psych(
    # Objeto generado por `fa` sobre el conjunto de entrenamiento
    df_train_fa_varimax,

    # Datos a los que queremos aplicar la transformacion
    df_test[, 1:8],

    # Datos sobre los que se ha aprendido la transformacion
    df_train[1:8]
)

# Ahora, añadimos la variable de salida que no participa en el analisis factorial
# Añadimos tambien los nombres de las columnas
df_test_fa <- cbind(df_test_fa, df_test$CLASS)
colnames(df_test_fa) <- c("MR1", "MR2", "CLASS")
```

## Estudiar el supuesto de normalidad multivariante

La clasificación usando discriminante lineal y cuadrático necesitan del supuesto de normalidad multivariante, entre otros. Así que en este apartado analizaremos dicho supuesto.

En [Estudio del supuesto de normalidad] ya hemos visto que **la normalidad univariante falla para todas las variables** de nuestro conjunto de datos. Así que estamos prácticamente seguros de que la normalidad multivariante fallará para nuestro conjunto de datos. Aún sabiendo que este supuesto va a fallar, lo comprobamos. Además, lo haremos para nuestros **tres conjuntos de datos** con los que trabajamos hasta ahora:

1. Conjunto de datos original, con cierta limpieza previa (borrado de datos faltantes, borrado de *outliers* y escalado de las variables)
2. Conjunto de datos original al que aplicamos PCA
3. Conjunto de datos original al que aplicamos Análisis Factorial

Esto lo hacemos porque realizaremos clasificación con los tres modelos, usando *k-Fold Cross Validation* para ver con qué conjunto de datos obtenemos mejores resultados, sin caer en *data snooping*.

### Contraste de hipótesis

H0: Las variables siguen una distribución normal multivariante

Empezamos con el **test de Mardia**:

```{r}
mardia_test <- mvn(data = df_train[, 1:8], mvnTest = "mardia", multivariatePlot = "qq")
mardia_test$multivariateNormality
```

Ahora, hacemos el **test de Henze-Zirkler**

```{r}
hz_test <- mvn(data = df_train[, 1:8], mvnTest = "hz")
hz_test$multivariateNormality
```

Podemos ver que ambos *tests* fallan, y por tanto, **contundentemente confirmamos que no tenemos normalidad multivariante**. Por tanto, esperamos que el rendimiento del discriminante cuadrático sea mejor que el del discriminante lineal, que es más sensible a la falta de esta normalidad multivariante.

Esto con el conjunto de datos original. Veamos ahora el caso de los datos transformados usando *PCA*. Como los datos originales no tienen normalidad multivariante, esperamos que los datos transformados sigan sin tener normalidad multivariante:

```{r}
# Aplicamos los dos tests
mardia_test <- mvn(data = df_train_pca[, 1:2], mvnTest = "mardia", multivariatePlot = "qq")
hz_test <- mvn(data = df_train_pca[, 1:2], mvnTest = "hz")

# Mostramos los resultados
mardia_test$multivariateNormality
hz_test$multivariateNormality
```

Ambos *tests* nos dicen que **no tenemos normalidad multivariante para los datos transformados con PCA**. Veamos ahora lo mismo para los datos transformados con el análisis factorial (tampoco esperamos que haya normalidad multivariante):

```{r}
# Aplicamos los dos tests
mardia_test <- mvn(data = df_train_fa[, 1:2], mvnTest = "mardia", multivariatePlot = "qq")
hz_test <- mvn(data = df_train_fa[, 1:2], mvnTest = "hz")

# Mostramos los resultados
mardia_test$multivariateNormality
hz_test$multivariateNormality
```

Y como era de esperar, **no tenemos normalidad multivariante para los datos transformados con Análisis Factorial**.

Por tanto, en **ninguno de los tres _datasets_ con los que trabajamos tenemos normalidad multivariante**, como era de esperar.

## Análisis de la homogeneidad de la varianza

Otra hipótesis necesaria para aplicar discriminante lineal y cuadrática es la homogeneidad de la varianza. Como tenemos varios predictores, usaremos la función `boxM` como una extensión del **test de Bartlett** para un solo predictor.

Este test es **muy sensible** al supuesto de normalidad multivariante, que ya sabemos que falla. Así que los resultados pueden ser del no todo fiables. Pero como ya sabemos que no tenemos normalidad multivariante, independientemente de que esta hipótesis de homogeneidad de la varianza se mantenga o no, la clasificación con discriminante (lineal y cuadrático) no funcionarán de forma óptima.

De nuevo, realizaremos este test para los **tres conjuntos de datos** con los que estamos trabajando. Empezando con el conjunto de datos original:

```{r}
boxM(data = df_train[, 1:8], grouping = dont_use_factor(df_train)[, 9])
```

Como *p-value* es $\approx 0 << 0.001$, rechazamos la hipótesis nula, y por tanto **no asumimos la homogeneidad de varianzas**.

Ahora, para el conjunto de datos transformado usando PCA:

```{r}
boxM(data = df_train_pca[, 1:2], grouping = df_train_pca[, 3])
```

De nuevo, como *p-value* es $\approx 0 << 0.001$, rechazamos la hipótesis nula, y por tanto no asumimos la homogeneidad de varianzas.

Y para finalizar, el conjunto de datos transformado usando Análisis Factorial:

```{r}
boxM(data = df_train_fa[, 1:2], grouping = df_train_fa[, 3])
```

Ocurre exactamente lo mismo, no asumimos la homogeneidad de la varianza.


> Recordar que no tenemos normalidad multivariante, y por tanto, las conclusiones sacadas al aplicar estos *tests* no son del todo fiables. Pero a la hora de aplicar discriminante lineal o cuadrático, ya con la falta de normalidad multivariante podemos pensar que no van a actuar de forma eficiente, independientemente de que haya o no homogeneidad de la varianza

----------------------------------------------------------------------------------------------------

# Exploración de hiperparámetros

Tenemos algunos hiperparámetros que escoger. Entre ellos:

1. Consideraremos tres modelos: discriminante lineal, discriminante cuadrático y *XGBOOST*^[Se puede consultar más inforación sobre este modelo en la [documentación oficial](https://xgboost.readthedocs.io/en/stable/index.html)]. Tenemos que elegir, al final, cuál de los tres modelos funciona mejor (aunque entrenaremos y validaremos los tres)
2. Tenemos también tres conjuntos de datos: el original que hemos limpiado ligeramente, el que se obtiene de aplicar PCA y finalmente, el que se obtiene de aplicar Análisis Factorial. En cada uno de los tres modelos, tenemos que ver qué conjunto de datos funciona mejor
3. *XGBOOST* tiene algunos hiperparámetros que podemos explorar, aunque en esta práctica no lo haremos

Para encontrar estos hiperparámetros, podríamos realizar el entrenamiento y ver qué resultados son los mejores en test. Esto supone **data snooping**, porque estamos involucrando el conjunto de test en el aprendizaje (aunque el último paso se haga por parte de una persona, sigue siendo parte del entrenamiento de un modelo).

Así que realizamos *k-Fold Cross Validation*. Partimos nuestro conjunto de datos en $k$ subconjuntos. Realizamos $k$ veces el siguiente proceso:

1. Consideramos un conjunto nuevo de entrenamiento con $k-1$ subconjuntos
2. Entrenamos sobre dicho conjunto de entrenamiento nuevo
3. Usamos el conjunto restante, que no hemos usado, para computar una métrica a optimizar

Una vez hecho eso, promediamos las $k$ métricas que estamos optimizando. Con esto, estamos usando todo el conjunto de datos de entrenamiento, tanto para entrenar como para validar, sin caer en *data snooping*.

Así que haremos esto para elegir los hiperparámetros ya comentados previamente.

Por ser un conjunto de datos pequeños, y al estar usando modelos rápidos de entrenar, tomamos un valor de $k$ relativamente elevado (cuanto mayor sea, mejor, pero supone que es más lento el proceso):

```{r}
number_of_folds <- 10
```

Además, siempre usaremos la siguiente fórmula, para los datos originales:

```{r}
form_original <- as.formula("CLASS ~ APARIENCIA + `FORMAS HABLAR` + `CONDICION FISICA` + `AGILIDAD MENTAL` + CONFIANZA + `HAB. PRESENTAR_IDEAS` + `HABILIDADES COMUNICACION` + `RENDIMIENTO ACADEMICO`")
```

Y siempre el mismo tipo de control:

```{r}
control <- trainControl(method = "cv", number = number_of_folds)
```

## Discriminante Lineal

**Datos originales**:

```{r}
cv_results <- train(form_original, data = df_train, trControl = control, method = "lda")
cv_results$results$Accuracy
```

**Datos PCA**:

```{r}
# Usamos las clases del dataframe original porque son las mismas y,
# principalmente, porque tienen el factor asociado que nos hace falta para este
# metodo
cv_results <- train(df_train_pca[, 1:2], df_train$CLASS, trControl = control, method = "lda")
cv_results$results$Accuracy
```

**Datos FA**:

```{r}
# Usamos las clases del dataframe original porque son las mismas y,
# principalmente, porque tienen el factor asociado que nos hace falta para este
# metodo
cv_results <- train(df_train_fa[, 1:2], df_train$CLASS, trControl = control, method = "lda")
cv_results$results$Accuracy
```

## Discriminante cuadrático

**Datos originales**:

```{r}
cv_results <- train(form_original, data = df_train, trControl = control, method = "qda")
cv_results$results$Accuracy
```

**Datos PCA**:

```{r}
# Usamos las clases del dataframe original porque son las mismas y,
# principalmente, porque tienen el factor asociado que nos hace falta para este
# metodo
cv_results <- train(df_train_pca[, 1:2], df_train$CLASS, trControl = control, method = "qda")
cv_results$results$Accuracy
```

**Datos FA**:

```{r}
# Usamos las clases del dataframe original porque son las mismas y,
# principalmente, porque tienen el factor asociado que nos hace falta para este
# metodo
cv_results <- train(df_train_fa[, 1:2], df_train$CLASS, trControl = control, method = "qda")
cv_results$results$Accuracy
```

## XGBOOST

> Para usar este modelo tenemos que generar *datasets* con un tipo concreto para este modelo, que es lo que hacemos en cada caso, para cada uno de los tres tipos de conjunto de datos que estamos manejando

En todos los casos, usamos los **mismos parámetros del modelo**, que no vamos a buscar optimizar con *k-Fold Cross Validation*:

```{r}
number_rounds <- 100
param <- list(max.depth = 10, eta = 0.3, nthread = 2, nrounds = number_rounds, objective = "binary:logistic")
```

**Datos originales**:

```{r}
# Convertimos los datos al formato que necesita xgboost
# Usamos una formula para indicar que no se use la variable objetivo
xgboost_train <- model.matrix(CLASS~.-1, data = df_train)

# La label tiene que ser valores 0, 1 para xgboost
zero_one_labels <- dont_use_factor(df_train)$CLASS - 1

# Ahora juntamos las variables de entrada, con las etiquetas formateadas
xgboost_train <- xgb.DMatrix(xgboost_train, label = zero_one_labels)

# Ejecutamos cross validation
results <- xgb.cv(
    param,
    xgboost_train,
    nrounds = number_rounds,
    nfold = number_of_folds,
    metrics = "error",

    # No nos interesan ahora ver las desviaciones
    showsd = FALSE
)

# Mostramos la media de los rounds de cross validation
# Este metodo devuelve el error, asi que tenemos que calcular el accuracy
error <- mean(results$evaluation_log$test_error_mean)
accuracy <- 1 - error
accuracy
```

**Datos PCA**:

```{r}
# Convertimos los datos al formato que necesita xgboost
# Usamos una formula para indicar que no se use la variable objetivo
xgboost_train <- model.matrix(CLASS~.-1, data = as.data.frame(df_train_pca))

# No generamos de nuevo las etiquetas, puesto que son las mismas que en el caso
# anterior

# Ahora juntamos las variables de entrada, con las etiquetas formateadas
xgboost_train <- xgb.DMatrix(xgboost_train, label = zero_one_labels)

# Ejecutamos cross validation
results <- xgb.cv(
    param,
    xgboost_train,
    nrounds = number_rounds,
    nfold = number_of_folds,
    metrics = "error",

    # No nos interesan ahora ver las desviaciones
    showsd = FALSE
)

# Mostramos la media de los rounds de cross validation
# Este metodo devuelve el error, asi que tenemos que calcular el accuracy
error <- mean(results$evaluation_log$test_error_mean)
accuracy <- 1 - error
accuracy
```

**Datos AF**:

```{r}
# Convertimos los datos al formato que necesita xgboost
# Usamos una formula para indicar que no se use la variable objetivo
xgboost_train <- model.matrix(CLASS~.-1, data = as.data.frame(df_train_fa))

# No generamos de nuevo las etiquetas, puesto que son las mismas que en el caso
# anterior

# Ahora juntamos las variables de entrada, con las etiquetas formateadas
xgboost_train <- xgb.DMatrix(xgboost_train, label = zero_one_labels)

# Ejecutamos cross validation
results <- xgb.cv(
    param,
    xgboost_train,
    nrounds = number_rounds,
    nfold = number_of_folds,
    metrics = "error",

    # No nos interesan ahora ver las desviaciones
    showsd = FALSE
)

# Mostramos la media de los rounds de cross validation
# Este metodo devuelve el error, asi que tenemos que calcular el accuracy
error <- mean(results$evaluation_log$test_error_mean)
accuracy <- 1 - error
accuracy
```

## Tabla resumiendo los resultados obtenidos

| Modelo                   | Dataset  | Accuracy      |
| :---                     | :---     | :---          |
| Discriminante Lineal     | Original | 0.5915605     |
| Discriminante Lineal     | PCA      | 0.5334985     |
| Discriminante Lineal     | FA       | 0.5284333     |
| Discriminante Cuadrático | Original | 0.6891568     |
| Discriminante Cuadrático | PCA      | 0.5615132     |
| Discriminante Cuadrático | FA       | 0.587775      |
| XGBOOST                  | Original | 0.9014924     |
| **XGBOOST**              | **PCA**  | **0.9046639** |
| XGBOOST                  | FA       | 0.9036134     |

## Conclusiones

1. Como comentaremos más adelante, en vista de que los supuestos fallan para los modelos de discriminante lineal y cuadrático, era de esperar que el discriminante cuadrático funcionase mejor (porque es más robusto ante la falta de los supuestos)
2. Un modelo basado en *boosting* obtiene los mejores resultados. Era de esperar porque es uno de los modelos más potentes, que siempre se suelen considerar a la hora de trabajar con datos tabulares.
3. Además, *XGBOOST* es el único modelo en el que la reducción de la dimensionalidad mejora los resultados. Además, se mejoran tanto usando *PCA* como usando *FA*

----------------------------------------------------------------------------------------------------

# Clasificación

> Aunque tenemos tres conjuntos de datos (original, PCA y FA), solo usaremos uno por cada modelo. Este conjunto de datos se escogerá como el que mejor resultados hemos visto en [Tabla resumiendo los resultados obtenidos]

## Clasificador con discriminante lineal

Ya hemos realizado un estudio sobre los supuestos que se tienen que verificar para que este clasificador funcione correctamente. Los dos supuestos principales (normalidad multivariante y homogeneidad de la varianza) no se sostienen, y por tanto, podemos espera que este modelo no funcione correctamente. Además, sabemos que este clasificador es más sensible frente a la falla de los supuestos que el discriminante cuadrático. Esto lo hemos podido comprobar en [Tabla resumiendo los resultados obtenidos]

En este caso, el mejor modelo obtenido es sobre el conjunto de datos original, sobre el que hemos realizado un procesado básico de los datos:

```{r}
modelo_lda <- lda(formula = form_original, data = df_train)
modelo_lda
```

Ahora, podemos usar los coeficientes del modelo para intuir cuál es la importancia de las variables a la hora de predecir la empleabilidad:

```{r}
# Generamos un dataframe con los datos que necesitamos para el grafico
values <- as.list(modelo_lda$scaling)
values <- sapply(values, abs) # Queremos los coeficientes en valor absoluto
names <- colnames(df_train[, 1:8])
plot_data <- data.frame(unlist(values), unlist(names))
colnames(plot_data) <- c("Coeficientes", "Variables")

# Ordenamos de mayor a menor en funcion del valor de los coeficientes
plot_data <- plot_data[order(plot_data$Coeficientes, decreasing = FALSE), ]

# Usamos esto para hacer un barplot
# `par` y `las=1` para que se muestren los nombres de las variables
# Ademas, añado margenes para que se muestren correctamente los nombres de las
# variables
par(mai=c(1,3,1,1))
barplot(plot_data$Coeficientes, names = plot_data$Variables, horiz = TRUE, las = 1, xpd = FALSE)
```

Con esto, podemos ver que:

1. Es un modelo mucho más justo de lo que pensábamos que iba a ser, por la exploración realizada previamente
2. La agilidad mental ,formas de hablar y presentar ideas son las tres variables más importantes, con una gran diferencia respecto a la cuarta (la confianza)
3. Las variable menos relevante es la apariencia, lo que parece indicarnos que es un modelo bastante justo
4. La siguiente menos relevante es el rendimiento académico
    - Esto ya lo esperábamos por la exploración previa
    - A la hora de seleccionar candidatos, esta variable creemos que debería ser más relevante
5. La condición física creemos que está demasiado valorada, haciendo al modelo menos justo
    - Está por encima de otras variables como las habilidades de comunicación y rendimiento académico
6. En general, tenemos un modelo que usa variables de una forma más o menos justa, con sus cosas buenas y malas. Aunque pensábamos que iba a ser un modelo mucho más injusto
   > Notar que la injusticia del modelo simplemente representa la injusticia de los datos con los que se ha construido. En ningún momento estamos diciendo que sea un modelo inherentemente injusto

## Clasificador con discriminante cuadrático

Ya hemos realizado un estudio sobre los supuestos que se tienen que verificar para que este clasificador funcione correctamente. Los dos supuestos principales (normalidad multivariante y homogeneidad de la varianza) no se sostienen, y por tanto, podemos espera que este modelo no funcione correctamente. Además, sabemos que este clasificador es más robusto frente a la falla de los supuestos que el discriminante cuadrático. Y por tanto es esperable que funcione mejor. En [Tabla resumiendo los resultados obtenidos] ya hemos visto que esto es así

En este caso, el mejor modelo obtenido es sobre el conjunto de datos original, sobre el que hemos realizado un procesado básico de los datos:

```{r}
modelo_qda <- qda(formula = form_original, data = df_train)
modelo_qda
```

En este caso, como tenemos una fórmula cuadrática, en vez de lineal, la interpretación del valor de los coeficientes no es tan directa como en el caso del discriminante lineal. Así que no realizamos una interpretación similar. Además, en [Clasificador XGBOOST], también hacemos una interpretación de este tipo, así que con ello tendremos suficiente información.

## Clasificador XGBOOST

Como hemos visto en [Tabla resumiendo los resultados obtenidos], este es el mejor modelo. El *dataset* sobre el que mejores resultados se obtiene es al que aplicamos *PCA*. Procedemos a entrenar el modelo sobre todo el conjunto de datos:

```{r}
# Convertimos los datos al formato que necesita xgboost
# Usamos una formula para indicar que no se use la variable objetivo
xgboost_train <- model.matrix(CLASS~.-1, data = as.data.frame(df_train_pca))

# No generamos de nuevo las etiquetas, puesto que son las mismas que en
# validacion cruzada anterior

# Ahora juntamos las variables de entrada, con las etiquetas formateadas
xgboost_train <- xgb.DMatrix(xgboost_train, label = zero_one_labels)

# Entrenamos el modelo
xgb_model <- xgb.train(
    data = xgboost_train,
    max_depth = 10,
    eta = 0.3,
    nrounds = 100,
    nthread = 2,
    objective = "binary:logistic",
    eval_metric = "error"
)
xgb_model
```

Como hacemos con los otros modelos, podemos usar el modelo entrenado para saber cuál es la importancia de las variables de entrada a la hora de realizar las predicciones (esto ya lo trae pre-programado el módulo de *XGBOOST*):

```{r}
imp_matrix <- xgb.importance(feature_names = colnames(xgboost_train), model = xgb_model)
print(xgb.plot.importance(importance_matrix = imp_matrix))
```

Como sospechábamos, la componente principal relacionada con el rendimiento académico es menos relevante que la que combina el resto de variables. Podemos hacer lo mismo con el el *dataframe* original, aunque no suponga el mejor modelo:

```{r}
# Convertimos los datos al formato que necesita xgboost
# Usamos una formula para indicar que no se use la variable objetivo
xgboost_train_complete <- model.matrix(CLASS~.-1, data = as.data.frame(df_train))

# No generamos de nuevo las etiquetas, puesto que son las mismas que en
# validacion cruzada anterior

# Ahora juntamos las variables de entrada, con las etiquetas formateadas
xgboost_train_complete <- xgb.DMatrix(xgboost_train_complete, label = zero_one_labels)

# Entrenamos el modelo
xgb_model_complete <- xgb.train(
    data = xgboost_train_complete,
    max_depth = 10,
    eta = 0.3,
    nrounds = 100,
    nthread = 2,
    objective = "binary:logistic",
    eval_metric = "error"
)

# Mostramos la importancia de las variables
imp_matrix <- xgb.importance(feature_names = colnames(xgboost_train_complete), model = xgb_model_complete)
print(xgb.plot.importance(importance_matrix = imp_matrix))
```

Con esto, podemos sacar las siguientes conclusiones:

1. Las dos variables más importantes son la agilidad mental y la apariencia. Nos sorprende que la segunda variable más importante a la hora de determinar la empleabilidad, *según este modelo*, sea la apariencia
2. En tercer lugar tenemos el rendimiento académico. En parte sorprende, porque como hemos ido comentando a lo largo de este cuaderno, parecía tener muy poca relevancia a la hora de determinar la empleabilidad. Puede ser que, como estamos trabajando con árboles de decisión (aunque tengamos *boosting* agregando múltiples árboles), cuando la agilidad mental y apariencia no sea suficiente para determinar la empleabilidad, el rendimiento académico sea una buena métrica para desempatar
3. Podemos pensar que es un modelo más o menos justo:
    - La agilidad mental y la apariencia, aunque puedan haber opiniones, son algo meritocráticas (un empleado con agilidad mental es preferible, la apariencia en parte depende del interés del candidato por el puesto de trabajo^[De nuevo, esto está completamente abierto a la opinión del lector])
    - La condición físicas y formas de hablar aparecen como las dos variables menos relevantes, y podríamos pensar que son las dos variables menos meritocráticas

----------------------------------------------------------------------------------------------------

# Validación

> En parte, ya hemos validado los modelos con *k-fold Cross Validation*. Sin embargo, validamos ahora los modelos entrenados sobre todo el conjunto de datos (y no sobre 9 *folds*), usando el valor de *accuracy* y viendo la matriz de confusión. Aunque sabemos que el modelo que mejor funciona es *XGBOOST*, realizamos la validación sobre los tres modelos

> **IMPORTANTE**: ya hemos elegido, con *k-fold Cross Validation* es el mejor modelo. Usar los resultados de la validación sobre *test* para escoger cuál es el mejor modelo implicaría que usamos estas métricas en el aprendizaje, cayendo en *data snooping*. Viendo los resultados de los tres modelos solo nos sirve para, o bien confirmar lo que hemos afirmado previamente, o ver que no es correcto y que por tanto algo en nuestra metodología ha fallado

## Discriminante lineal

Empezamos viendo el **accuracy** del modelo, tanto en entrenamiento como en *test*. Aunque nos interesa sobre todo el rendimiento en *test*, viendo también el rendimiento en entrenamiento podemos ver ciertos problemas (i.e. *overfitting*). Empezamos con esta función auxiliar, para no repetir el mismo código múltiples veces:

```{r}
# Calcula el accuracy de un modelo sobre un conjunto de datos
# Tambien calcula la matriz de confusion
# `input_vars` data.frame con las variables de entrada
# `labels` vector con las etiquetas reales
# `model` modelo del que calculamos el accuracy y la matriz de confusion
get_eval_metrics <- function(input_vars, labels, model) {
    # Realizamos las predicciones usando el modelo y variables de entrada
    predictions <- predict(model, input_vars)

    # Miramos cuantas predicciones son correctas
    correct_predictions <- predictions$class == labels

    # Lo usamos para computar el accuracy
    accuracy <- sum(correct_predictions) / length(correct_predictions)

    # Veamos la matriz de confusion
    conf_matrix <- confusionMatrix(data = predictions$class, reference = labels)

    # Devolvemos las dos metrics en una lista nombrada
    return(list(accuracy = accuracy, conf_matrix = conf_matrix))
}
```

Más tarde visualizaremos la matriz de confusión con la siguiente función auxiliar:

```{r}
# Dada una matriz de confusion, la muestra graficamente
# Copio y modifico la funcion del siguiente recurso:
#   https://stackoverflow.com/a/64539733
plot_confusion_matrix <- function(conf_matrix) {

    # Hago esta adaptacion para que funcion el siguiente codigo
    cm <- conf_matrix

    # Este codigo esta copiado directamente de stackoverflow
    plt <- as.data.frame(cm$table)
    plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))

    ggplot(plt, aes(Prediction,Reference, fill= Freq)) +
            geom_tile() + geom_text(aes(label=Freq)) +
            scale_fill_gradient(low="white", high="#009194") +
            labs(x = "Reference",y = "Prediction") +
            scale_x_discrete(labels=c("Empleable", "No Empleable")) +
            scale_y_discrete(labels=c("Empleable", "No Empleable"))
}
```

Calculamos ahora las métricas:

```{r}
# Calculamos las metricas
train_metrics <- get_eval_metrics(df_train[, 1:8], df_train$CLASS, modelo_lda)
test_metrics <- get_eval_metrics(df_test[, 1:8], df_test$CLASS, modelo_lda)
```

Mostramos el accuracy:

```{r}
message("Train accuracy: ", train_metrics$accuracy, "  Test accuracy: ", test_metrics$accuracy)
```

Mostramos las matrices de confusión:

```{r}
message("Entrenamiento")
train_metrics$conf_matrix


message("Test")
test_metrics$conf_matrix
```

Visualicemos la matriz de confusión en *test*:

```{r}
plot_confusion_matrix(test_metrics$conf_matrix)
```

## Discriminante cuadrático

Usamos el código anterior para hacer lo mismo con este modelo:

```{r}
# Calculamos las metricas
train_metrics <- get_eval_metrics(df_train[, 1:8], df_train$CLASS, modelo_qda)
test_metrics <- get_eval_metrics(df_test[, 1:8], df_test$CLASS, modelo_qda)
```

Mostramos el accuracy:

```{r}
message("Train accuracy: ", train_metrics$accuracy, "  Test accuracy: ", test_metrics$accuracy)
```

Mostramos las matrices de confusión:

```{r}
message("Entrenamiento")
train_metrics$conf_matrix


message("Test")
test_metrics$conf_matrix
```

Visualicemos la matriz de confusión en *test*:

```{r}
plot_confusion_matrix(test_metrics$conf_matrix)
```

## XGBOOST

En este caso, tenemos que modificar el código que usamos para el cálculo del *accuracy*:

```{r}
# Realizamos la conversion de datos, que en test todavia no hemos realizado
xgboost_test_data <- model.matrix(CLASS~.-1, data = as.data.frame(df_test_pca))
zero_one_labels_test <- dont_use_factor(df_test_pca)$CLASS - 1
xgboost_test <- xgb.DMatrix(xgboost_test_data, label = zero_one_labels_test)

# Realizamos las predicciones en test
test_predictions <- predict(xgb_model, xgboost_test_data)
test_predictions <- as.numeric(test_predictions > 0.5)
correct_predictions <- test_predictions == zero_one_labels_test
test_accuracy <- sum(correct_predictions) / length(correct_predictions)

# Ahora hacemos lo mismo para train
train_predictions <- predict(xgb_model, xgboost_train)
train_predictions <- as.numeric(train_predictions > 0.5)
correct_predictions <- train_predictions == zero_one_labels
train_accuracy <- sum(correct_predictions) / length(correct_predictions)

# Ya podemos mostrar los resultados
message("Train accuracy: ", train_accuracy, "  Test accuracy: ", test_accuracy)
```

Y ahora, podemos calcular la matriz de confusión:

```{r}
# Calculamos las matrices
# Aplicamos factores a las etiquetas 0, 1 porque asi lo requiere la funcion
conf_matrix_train <- confusionMatrix(data = factor(train_predictions), reference = factor(zero_one_labels))
conf_matrix_test <- confusionMatrix(data = factor(test_predictions), reference = factor(zero_one_labels_test))

# Mostramos las matrices
message("Entrenamiento")
conf_matrix_train

message("Test")
conf_matrix_test
```

Visualicemos la matriz de confusión en *test*:

```{r}
plot_confusion_matrix(test_metrics$conf_matrix)
```

## Resumen de los resultados obtenidos

Empezamos viendo los __valores de *accuracy*__:

| Modelo                   | Train acc         | Test acc          |
| :---                     | :---              | :---              |
| Discriminante lineal     | 0.591178965224767 | 0.598319327731092 |
| Discriminante cuadrático | 0.706106870229008 | 0.697478991596639 |
| XGBOOST                  | 0.913910093299406 | 0.890756302521008 |

Con esto, podemos ver que:

1. *k-fold Cross Validation* ha sido efectivo, puesto que *XGBOOST* es efectivamente el modelo que mejor comportamiento ha mostrado
2. Además, la métrica obtenida en *test* no está demasiado alejada de la estimada usando *k-fold Cross Validation* (menos de un $0.01$ de diferencia)
3. A pesar de las fallas de supuestos, el discriminante cuadrático ha obtenido unos resultados decentes, con un *accuracy* casi del 70%
4. Nuestros modelos no tienen *overfitting*, y por tanto, podemos pensar que van a generalizar bien

Ahora, pasemos a comentar las **matrices de confusión**. Comentaremos solo las matrices sobre conjunto de test, puesto que con los valores de *accuracy* ya hemos visto que nuestros modelos generalizan bien. Aunque podamos usar estas matrices para calcular las métricas recall, precision, specificity, f-score, accuracy y área bajo la curva ROC, con esto y los comentarios que vamos a realizar será suficiente (aunque en ocasiones nos refiramos a alguno de estos valores de forma implícita).

**Discriminante lineal**:

| Predicción / Valor real | Empleable | No empleable |
| :---                    | :---      | :---         |
| Empleable               | 85        | 80           |
| No empleable            | 159       | 271          |

Con esto, podemos ver que:

1. Tenemos muchos falsos negativos, muchos más que falsos positivos. Esto en el ámbito en el que nos encontramos no es del todo crítico, puesto que estamos rechazando más de la cuenta (en otros ambientes esto puede ser peligroso)
2. Tenemos unos pocos falsos positivos. Estos errores son más relevantes que los falsos negativos, puesto que no queremos que en nuestra empresa entren candidatos no adecuados
3. No es un modelo demasido fiable, principalmente, porque los valores fuera de la diagonal ($159 + 80 = 239) está muy cercana a los valores de la diagonal (85 + 271 = 356)
4. El modelo tiende a clasificar como no empleables (165 positivos vs 431 negativos, lo que supone una relacion 27%-73%, cuando sabíamos que hay aproximadamente 40% positivos 60% negativos en nuestros datos)
5. Por tanto, podemos sospechar que el modelo casi siempre clasifica como no empleables. El *accuracy* del modelo está algo por debajo del 60%, que es aproximadamente la proporción de no empleables del *dataset*. Sospechamos que el modelo no está realizando un aprendizaje efectivo, simplemente clasifica como negativos casi siempre

**Discriminante cuadrático**:

| Predicción / Valor real | Empleable | No empleable |
| :---                    | :---      | :---         |
| Empleable               | 121       | 57           |
| No empleable            | 123       | 294          |

Con esto podemos ver que:

1. Respecto al modelo *LDA* hemos mejorado, teniendo menos valores fuera de la diagonal (no muchos menos, la mejora ha sido incremental)
2. Seguimos teniendo cierta tendencia a clasificar más como no empleables (243 empleables vs 352 no empleables). Ahora ya sí que estamos en una relación 40-60 (casi perfecta), y por tanto podemos pensar que el modelo sí está realizando un aprendizaje (*algo*) efectivo de los datos
3. Sin embargo, seguimos teniendo demasiados valores fuera de la diagonal
4. Como en el modelo anterior, son mucho más acentuados los falsos negativos que los falsos positivos, lo cual hace menos graves los fallos en el contexto en el que nos encontramos

**XGBOOST**:

| Predicción / Valor real | No empleable | Empleable |
| :---                    | :---         | :---      |
| No empleable            | 211          | 32        |
| Empleable               | 33           | 319       |

Con esto podemos ver que:

1. Apenas tenemos valores fuera de la diagonal, la mejora respecto a los dos modelos previos es abrupta
2. Como tenemos muy pocos valores fuera de la diagonal, podemos pensar que el modelo ha realizado un aprendizaje bastante efectivo del problema presentado
3. Tenemos prácticamente los mismos falsos positivos que falsos negativos
4. No comentamos las tendencias del modelo hacia una clasificación más hacia la empleabilidad o hacia la no empleabilidad, puesto que apenas tenemos fallos y el modelo realiza un trabajo efectivo

----------------------------------------------------------------------------------------------------

# Experimento Adicional

## Motivación

A lo largo de este cuaderno hemos podido ver que, quizás, el proceso de selección no es del todo justo. Esto es, que el hecho de aceptar o no a una persona puede depender de factores poco meritocráticos (i.e. la condición físicas).

Por ejemplo, hemos visto la poca relevancia del rendimiento académico en algunos modelos, cuando debería ser una de las variables más relevantes.

Para confirmar esto, vamos a realizar un breve experimento. Aunque se podría plantear de forma mucho más profunda, por el alcance de esta práctica final no realizamos dicho estudio en profundidad.

El experimento consistirá en entrenar dos modelos, uno usando variables (que consideramos) meritocráticas, y otro con variables (de nuevo, que consideramos) no meritocráticas. Si el modelo que usa variables no meritocráticas obtiene mejores resultados a la hora de realizar las prediccciones, podremos pensar que estas variables no meritocráticas explican mejor el proceso mental llevado a cabo por los empleadores, y por tanto, el proceso parecerá ser más injusto.

Como hemos visto que el mejor modelo es *XGBOOST*, y como nos da algunas facilidades para interpretar los modelos obtenidos, usaremos este modelo. Además, usaremos el conjunto de datos original para simplificar el desarrollo del experimento. Esto porque, para aplicar PCA, habría que aplicarlo a los dos conjuntos por separado, y ver si se obtienen mejores resultados que sin aplicar PCA, usando *k-fold Cross Validation*.

## Elección de los subconjuntos de datos

Ahora, pasamos a dividir nuestro conjunto de datos en dos: uno con variables meritocráticas y otro con variables no meritocráticas. Esta elección es arbitraria y totalmente subjetiva, lo normal es que el lector no esté totalmente de acuerdo con esta elección. El código de este cuaderno está pensado para que cambiando la siguiente división, todo funcione sin prácticamente cambios:

Como variables meritocráticas consideramos:

1. Rendimiento académico
2. Agilidad mental
3. Habilidad para presentar ideas
4. Habilidades de comunicación

Como variables no meritocráticas consideramos:

1. Condición físicas
2. Apariencia
3. Formas de hablar
4. Confianza

Con esto, realizamos la separación del conjunto de datos:

```{r}
# Variables meritocraticas
mer_vars <- c(4, 6, 7, 8)

# Variables no meritocraticas
non_mer_vars <- c(1, 2, 3, 5)

# Añadimos la variable de salida
mer_vars <- c(mer_vars, 9)
non_mer_vars <- c(non_mer_vars, 9)

# Realizamos la separacion de los datos

df_train_mer <- df_train[, mer_vars]
df_train_non_mer <- df_train[, non_mer_vars]
```

Veamos los primeros datos de cada uno de estos *dataframes* para asegurarnos de que la separación es correcta:

```{r}
head(df_train_mer)
head(df_train_non_mer)
```

## Ajuste de hiperparámetros para ambos modelos

Vamos a realizar un pequeño ajuste de los hiperparámetros de *XGBOOST* sobre ambos conjuntos de datos. Será una búsqueda de hiperparámetros usando la ténica de *Grid Search*^[Uso este [tutorial](https://www.projectpro.io/recipes/tune-hyper-parameters-grid-search-r) para implementar Grid Search]:

**Datos meritocráticos**:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Especificamos que vamos a usar 5 fold cross validation con grid search
# Usamos un valor menor de 10 porque la computacion ahora va a ser mas intensiva
train_control <- trainControl(method = "cv", number = 5, search = "grid")

# Especificamos el grid sobre el que nos vamos a mover
grid <- expand.grid(
    # Valores que exploramos
    max_depth = c(3, 5, 7),
    nrounds = (1:50) * 10,
    eta = c(0.1, 0.2, 0.3),

    # Valores que no exploramos
    # Colocamos los valores por defecto del modelo
    gamma = 0,
    subsample = 1,
    min_child_weight = 1,
    colsample_bytree = 0.6
)

# Realizamos la exploracion
grid_search_results_mer <- train(
    CLASS~.,
    data = df_train_mer,
    method = "xgbTree",
    trControl = train_control,
    tuneGrid = grid
)
```

**Datos no meritocráticos**:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Especificamos que vamos a usar 5 fold cross validation con grid search
# Usamos un valor menor de 10 porque la computacion ahora va a ser mas intensiva
train_control <- trainControl(method = "cv", number = 5, search = "grid")

# Especificamos el grid sobre el que nos vamos a mover
grid <- expand.grid(
    # Valores que exploramos
    max_depth = c(3, 5, 7),
    nrounds = (1:50) * 10,
    eta = c(0.1, 0.2, 0.3),

    # Valores que no exploramos
    # Colocamos los valores por defecto del modelo
    gamma = 0,
    subsample = 1,
    min_child_weight = 1,
    colsample_bytree = 0.6
)

# Realizamos la exploracion
grid_search_results_non_mer <- train(
    CLASS~.,
    data = df_train_non_mer,
    method = "xgbTree",
    trControl = train_control,
    tuneGrid = grid
)
```

En las dos celdas anteriores no mostramos el proceso, porque se imprimen muchas lineas con el *log* del proceso. Ahora pasamos a mostrar los resultados, empezando por el conjunto meritocrático:

```{r}
grid_search_results_mer
```

Y ahora, el conjunto no meritocrático:

```{r}
grid_search_results_non_mer
```

## Entrenamiento de los modelos

Ahora que tenemos los dos conjuntos de parámetros pseudo-óptimos (la exploración que hemos hecho no nos asegura que estos parámetros sean óptimos) pasamos a entrenar los dos modelos. Empezamos con el modelo meritocrático:

```{r}
# Convertimos los datos al formato que necesita xgboost
# Usamos una formula para indicar que no se use la variable objetivo
xgboost_train_mer <- model.matrix(CLASS ~ .-1, data = as.data.frame(df_train_mer))

# Usamos las etiquetas transformadas en el entrenamiento de XGBOOST original

# Ahora juntamos las variables de entrada, con las etiquetas formateadas
xgboost_train_mer <- xgb.DMatrix(xgboost_train_mer, label = zero_one_labels)

# Entrenamos el modelo usando los parametros optimos
xgb_model_mer <- xgb.train(
    data = xgboost_train_mer,
    max_depth = 3,
    eta = 0.3,
    nrounds = 480,
    nthread = 2,
    objective = "binary:logistic",
    eval_metric = "error"
)
xgb_model_mer
```

Hacemos lo mismo para el modelo no meritocrático:

```{r}
# Convertimos los datos al formato que necesita xgboost
# Usamos una formula para indicar que no se use la variable objetivo
xgboost_train_non_mer <- model.matrix(CLASS ~ .-1, data = as.data.frame(df_train_non_mer))

# Usamos las etiquetas transformadas en el entrenamiento de XGBOOST original

# Ahora juntamos las variables de entrada, con las etiquetas formateadas
xgboost_train_non_mer <- xgb.DMatrix(xgboost_train_non_mer, label = zero_one_labels)

# Entrenamos el modelo usando los parametros optimos
xgb_model_non_mer <- xgb.train(
    data = xgboost_train_non_mer,
    max_depth = 3,
    eta = 0.3,
    nrounds = 480,
    nthread = 2,
    objective = "binary:logistic",
    eval_metric = "error"
)
xgb_model_non_mer
```

## Resultados obtenidos

Ahora que tenemos los dos modelos entrenados, veamos su rendimiento sobre el conjunto de *test*. Para ello, primero necesitamos generar dichos conjuntos:

```{r}
df_test_mer <- df_test[, mer_vars]
df_test_non_mer <- df_test[, non_mer_vars]
```

Y ahora evaluamos, empezando en el modelo meritocrático:

```{r}
# Realizamos la conversion de datos al formato que XGBOOST espera
xgboost_test_data_mer <- model.matrix(CLASS~.-1, data = as.data.frame(df_test_mer))
zero_one_labels_test <- dont_use_factor(df_test_mer)$CLASS - 1
xgboost_test_data_mer <- xgb.DMatrix(xgboost_test_data_mer, label = zero_one_labels_test)

# Realizamos las predicciones en test
test_predictions <- predict(xgb_model_mer, xgboost_test_data_mer)
test_predictions <- as.numeric(test_predictions > 0.5)
correct_predictions <- test_predictions == zero_one_labels_test
test_accuracy_mer <- sum(correct_predictions) / length(correct_predictions)

# Ahora hacemos lo mismo para train
train_predictions <- predict(xgb_model_mer, xgboost_train_mer)
train_predictions <- as.numeric(train_predictions > 0.5)
correct_predictions <- train_predictions == zero_one_labels
train_accuracy_mer <- sum(correct_predictions) / length(correct_predictions)

# Ya podemos mostrar los resultados
message("Train accuracy: ", train_accuracy_mer, "  Test accuracy: ", test_accuracy_mer)
```

Hacemos lo mismo para el modelo no meritocrático:

```{r}
# Realizamos la conversion de datos al formato que XGBOOST espera
xgboost_test_data_non_mer <- model.matrix(CLASS~.-1, data = as.data.frame(df_test_non_mer))
zero_one_labels_test <- dont_use_factor(df_test_non_mer)$CLASS - 1
xgboost_test_data_non_mer <- xgb.DMatrix(xgboost_test_data_non_mer, label = zero_one_labels_test)

# Realizamos las predicciones en test
test_predictions <- predict(xgb_model_non_mer, xgboost_test_data_non_mer)
test_predictions <- as.numeric(test_predictions > 0.5)
correct_predictions <- test_predictions == zero_one_labels_test
test_accuracy_non_mer <- sum(correct_predictions) / length(correct_predictions)

# Ahora hacemos lo mismo para train
train_predictions <- predict(xgb_model_non_mer, xgboost_train_non_mer)
train_predictions <- as.numeric(train_predictions > 0.5)
correct_predictions <- train_predictions == zero_one_labels
train_accuracy_non_mer <- sum(correct_predictions) / length(correct_predictions)

# Ya podemos mostrar los resultados
message("Train accuracy: ", train_accuracy_non_mer, "  Test accuracy: ", test_accuracy_non_mer)
```

Resumimos los resultados en la siguiente tabla:

| Conjunto de datos | Train Acc         | Test Acc              |
| :---              | :---              | :---                  |
| **Meritocŕatico** | 0.733248515691264 | 0.694117647058824     |
| No meritocrático  | 0.719677692960136 | **0.715966386554622** |

## Conclusiones

<!-- Al final, siempre tenemos las notas a pie de página -->
<!-- Así que ponemos un título para que está sección se vea correctamente -->

# Notas a pie de página
